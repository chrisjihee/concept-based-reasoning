{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d680961880222a81",
   "metadata": {},
   "source": [
    "# **Getting to know Llama 3: Everything you need to start building**\n",
    "Our goal in this session is to provide a guided tour of Llama 3 with comparison with Llama 2, including understanding different Llama 3 models, how and where to access them, Generative AI and Chatbot architectures, prompt engineering, RAG (Retrieval Augmented Generation), Fine-tuning and more. All this is implemented with a starter code for you to take it and use it in your Llama 3 projects."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c545e17b77f35ab",
   "metadata": {},
   "source": [
    "### **0 - Prerequisites**\n",
    "* Basic understanding of Large Language Models\n",
    "* Basic understanding of Python"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-20T17:03:55.757957Z",
     "start_time": "2024-06-20T17:03:55.040232Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import base64\n",
    "from getpass import getpass\n",
    "\n",
    "from IPython.display import display, Image, Markdown, HTML\n",
    "\n",
    "from chrisbase.data import *\n",
    "from chrisbase.io import *\n",
    "from chrisbase.util import *"
   ],
   "id": "3a6cabd1ff6c35a7",
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "id": "d87dc3a5a98e8f7f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-20T17:03:55.762320Z",
     "start_time": "2024-06-20T17:03:55.758944Z"
    }
   },
   "source": [
    "def mm(graph):\n",
    "    graphbytes = graph.encode(\"ascii\")\n",
    "    base64_bytes = base64.b64encode(graphbytes)\n",
    "    base64_string = base64_bytes.decode(\"ascii\")\n",
    "    display(Image(url=\"https://mermaid.ink/img/\" + base64_string))\n",
    "\n",
    "\n",
    "def md(t):\n",
    "    display(Markdown(t))\n",
    "\n",
    "\n",
    "def hr():\n",
    "    display(HTML(\"<hr style='border:3px solid MidnightBlue; width:880px; padding:0;\"\n",
    "                 \" margin-left:0; margin-right:0; margin-top:10px; margin-bottom:15px;'>\"))\n",
    "\n",
    "\n",
    "def mds(ps: dict):\n",
    "    for k, v in ps.items():\n",
    "        hr()\n",
    "        md(f\"**[{k}]**\\n{prefixed_str(v, '> ')}\")\n",
    "    if ps:\n",
    "        hr()\n",
    "\n",
    "\n",
    "def llama2_family():\n",
    "    mm(\"\"\"\n",
    "    graph LR;\n",
    "        llama-2 --> llama-2-7b\n",
    "        llama-2 --> llama-2-13b\n",
    "        llama-2 --> llama-2-70b\n",
    "        llama-2-7b --> llama-2-7b-chat\n",
    "        llama-2-13b --> llama-2-13b-chat\n",
    "        llama-2-70b --> llama-2-70b-chat\n",
    "        classDef default fill:#CCE6FF,stroke:#84BCF5,textColor:#1C2B33,fontFamily:trebuchet ms;\n",
    "    \"\"\")\n",
    "\n",
    "\n",
    "def llama3_family():\n",
    "    mm(\"\"\"\n",
    "    graph LR;\n",
    "        llama-3 --> llama-3-8b\n",
    "        llama-3 --> llama-3-70b\n",
    "        llama-3-8b --> llama-3-8b-instruct\n",
    "        llama-3-70b --> llama-3-70b-instruct\n",
    "        classDef default fill:#CCE6FF,stroke:#84BCF5,textColor:#1C2B33,fontFamily:trebuchet ms;\n",
    "    \"\"\")\n",
    "\n",
    "\n",
    "def rag_arch():\n",
    "    mm(\"\"\"\n",
    "    flowchart TD\n",
    "        A[User Prompts] --> B(Frameworks e.g. LangChain)\n",
    "        B <--> |Database, Docs, XLS|C[fa:fa-database External Data]\n",
    "        B -->|API|D[Llama 3]\n",
    "        classDef default fill:#CCE6FF,stroke:#84BCF5,textColor:#1C2B33,fontFamily:trebuchet ms;\n",
    "    \"\"\")\n"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "id": "acd794f15ce1f743",
   "metadata": {},
   "source": "## **1 - Understanding Llama 3**"
  },
  {
   "cell_type": "markdown",
   "id": "548692b6a6dd66c",
   "metadata": {},
   "source": [
    "### **1.1 - What is Llama 3?**\n",
    "* State of the art (SOTA), Open Source LLM\n",
    "* 8B, 70B - base and instruct models\n",
    "* Choosing model: Size, Quality, Cost, Speed\n",
    "* Pretrained + Chat\n",
    "* [Meta Llama 3 Blog](https://ai.meta.com/blog/meta-llama-3/)\n",
    "* [Getting Started with Meta Llama](https://llama.meta.com/docs/get-started)"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-20T17:03:55.765818Z",
     "start_time": "2024-06-20T17:03:55.762933Z"
    }
   },
   "cell_type": "code",
   "source": "llama2_family()",
   "id": "997e5b590a3c478a",
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://mermaid.ink/img/CiAgICBncmFwaCBMUjsKICAgICAgICBsbGFtYS0yIC0tPiBsbGFtYS0yLTdiCiAgICAgICAgbGxhbWEtMiAtLT4gbGxhbWEtMi0xM2IKICAgICAgICBsbGFtYS0yIC0tPiBsbGFtYS0yLTcwYgogICAgICAgIGxsYW1hLTItN2IgLS0+IGxsYW1hLTItN2ItY2hhdAogICAgICAgIGxsYW1hLTItMTNiIC0tPiBsbGFtYS0yLTEzYi1jaGF0CiAgICAgICAgbGxhbWEtMi03MGIgLS0+IGxsYW1hLTItNzBiLWNoYXQKICAgICAgICBjbGFzc0RlZiBkZWZhdWx0IGZpbGw6I0NDRTZGRixzdHJva2U6Izg0QkNGNSx0ZXh0Q29sb3I6IzFDMkIzMyxmb250RmFtaWx5OnRyZWJ1Y2hldCBtczsKICAgIA==\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-20T17:03:55.768638Z",
     "start_time": "2024-06-20T17:03:55.766925Z"
    }
   },
   "cell_type": "code",
   "source": "llama3_family()",
   "id": "a2ef68ece727fa3f",
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://mermaid.ink/img/CiAgICBncmFwaCBMUjsKICAgICAgICBsbGFtYS0zIC0tPiBsbGFtYS0zLThiCiAgICAgICAgbGxhbWEtMyAtLT4gbGxhbWEtMy03MGIKICAgICAgICBsbGFtYS0zLThiIC0tPiBsbGFtYS0zLThiLWluc3RydWN0CiAgICAgICAgbGxhbWEtMy03MGIgLS0+IGxsYW1hLTMtNzBiLWluc3RydWN0CiAgICAgICAgY2xhc3NEZWYgZGVmYXVsdCBmaWxsOiNDQ0U2RkYsc3Ryb2tlOiM4NEJDRjUsdGV4dENvbG9yOiMxQzJCMzMsZm9udEZhbWlseTp0cmVidWNoZXQgbXM7CiAgICA=\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "id": "190b25e8cdc4c64d",
   "metadata": {},
   "source": [
    "### **1.2 - Accessing Llama 3**\n",
    "* Download + Self Host (i.e. [download Llama](https://ai.meta.com/resources/models-and-libraries/llama-downloads))\n",
    "* Hosted API Platform (e.g. [Groq](https://console.groq.com/), [Replicate](https://replicate.com/meta/meta-llama-3-8b-instruct), [Together](https://api.together.xyz/playground/language/meta-llama/Llama-3-8b-hf), [Anyscale](https://app.endpoints.anyscale.com/playground))\n",
    "* Hosted Container Platform (e.g. [Azure](https://techcommunity.microsoft.com/t5/ai-machine-learning-blog/introducing-llama-2-on-azure/ba-p/3881233), [AWS](https://aws.amazon.com/blogs/machine-learning/llama-2-foundation-models-from-meta-are-now-available-in-amazon-sagemaker-jumpstart/), [GCP](https://console.cloud.google.com/vertex-ai/publishers/google/model-garden/139))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7f0f1cf7c500be4",
   "metadata": {},
   "source": [
    "### **1.3 - Use Cases of Llama 3**\n",
    "* Content Generation\n",
    "* Summarization\n",
    "* General Chatbots\n",
    "* RAG (Retrieval Augmented Generation): Chat about Your Own Data\n",
    "* Fine-tuning\n",
    "* Agents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "965614383680fb8",
   "metadata": {},
   "source": [
    "## **2 - Using and Comparing Llama 3 and Llama 2**\n",
    "We will be using Llama 2 7b & 70b chat and Llama 3 8b & 70b instruct models hosted on [Replicate](https://replicate.com/search?query=llama) to run the examples here. \n",
    "You will need to first sign in with Replicate with your github account, then create a free API token [here](https://replicate.com/account/api-tokens) that you can use for a while. \n",
    "You can also use other Llama 3 cloud providers such as [Groq](https://console.groq.com/), [Together](https://api.together.xyz/playground/language/meta-llama/Llama-3-8b-hf), or [Anyscale](https://app.endpoints.anyscale.com/playground)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67f1a112c1342b73",
   "metadata": {},
   "source": [
    "### **2.1 - Install dependencies**"
   ]
  },
  {
   "cell_type": "code",
   "id": "8a77a8bb71e58ed5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-20T17:03:56.358590Z",
     "start_time": "2024-06-20T17:03:55.769105Z"
    }
   },
   "source": [
    "# Install dependencies and initialize\n",
    "!pip list | grep -E \"torch|faiss|langchain|transformers|huggingface|groq|replicate|chris\""
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chrisbase                 0.5.3\r\n",
      "faiss-cpu                 1.8.0\r\n",
      "groq                      0.9.0\r\n",
      "huggingface-hub           0.23.4\r\n",
      "langchain                 0.2.5\r\n",
      "langchain-community       0.2.5\r\n",
      "langchain-core            0.2.9\r\n",
      "langchain-groq            0.1.5\r\n",
      "langchain-huggingface     0.0.3\r\n",
      "langchain-text-splitters  0.2.1\r\n",
      "replicate                 0.26.0\r\n",
      "sentence-transformers     3.0.1\r\n",
      "torch                     2.3.1\r\n",
      "transformers              4.41.2\r\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### **2.2 - Create helpers for Llama 2 and Llama 3**\n",
    "First, set your API token as environment variables."
   ],
   "id": "da1eb02f1aa9b632"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-20T17:03:56.376658Z",
     "start_time": "2024-06-20T17:03:56.359800Z"
    }
   },
   "cell_type": "code",
   "source": [
    "logging.getLogger(\"IPKernelApp\").setLevel(logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "args = CommonArguments(\n",
    "    env=ProjectEnv(\n",
    "        project=\"LLM-based\",\n",
    "        job_name=\"LLaMA-based-Tutorial\",\n",
    "        msg_level=logging.INFO,\n",
    "        msg_format=LoggingFormat.PRINT_00,\n",
    "    )\n",
    ")\n",
    "args.dataframe()"
   ],
   "id": "3f33f0fc379c4f04",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "      CommonArguments                                                                                        value\n",
       "0                 tag                                                                                         None\n",
       "1         env.project                                                                                    LLM-based\n",
       "2        env.job_name                                                                         LLaMA-based-Tutorial\n",
       "3     env.job_version                                                                                         None\n",
       "4        env.hostname                                                                           ChrisBookPro.local\n",
       "5        env.hostaddr                                                                                  192.168.0.2\n",
       "6      env.time_stamp                                                                                  0621.020355\n",
       "7     env.python_path                                        /Users/chris/miniforge3/envs/LLM-based/bin/python3.11\n",
       "8     env.current_dir                                                       /Users/chris/PycharmProjects/LLM-based\n",
       "9    env.current_file                         /Users/chris/PycharmProjects/LLM-based/Getting_to_know_Llama_3.ipynb\n",
       "10    env.working_dir                                                       /Users/chris/PycharmProjects/LLM-based\n",
       "11   env.command_args  [-f, /Users/chris/Library/Jupyter/runtime/kernel-f7dc3fb7-1360-44e1-9eeb-14cb1e48f034.json]\n",
       "12   env.num_ip_addrs                                                                                            0\n",
       "13    env.max_workers                                                                                            1\n",
       "14      env.debugging                                                                                        False\n",
       "15      env.msg_level                                                                                           20\n",
       "16     env.msg_format                                                                                  %(message)s\n",
       "17    env.date_format                                                                             [%m.%d %H:%M:%S]\n",
       "18    env.output_home                                                                                         None\n",
       "19   env.logging_file                                                                                         None\n",
       "20  env.argument_file                                                                                         None\n",
       "21       time.started                                                                                         None\n",
       "22       time.settled                                                                                         None\n",
       "23       time.elapsed                                                                                         None"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CommonArguments</th>\n",
       "      <th>value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>tag</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>env.project</td>\n",
       "      <td>LLM-based</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>env.job_name</td>\n",
       "      <td>LLaMA-based-Tutorial</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>env.job_version</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>env.hostname</td>\n",
       "      <td>ChrisBookPro.local</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>env.hostaddr</td>\n",
       "      <td>192.168.0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>env.time_stamp</td>\n",
       "      <td>0621.020355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>env.python_path</td>\n",
       "      <td>/Users/chris/miniforge3/envs/LLM-based/bin/python3.11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>env.current_dir</td>\n",
       "      <td>/Users/chris/PycharmProjects/LLM-based</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>env.current_file</td>\n",
       "      <td>/Users/chris/PycharmProjects/LLM-based/Getting_to_know_Llama_3.ipynb</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>env.working_dir</td>\n",
       "      <td>/Users/chris/PycharmProjects/LLM-based</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>env.command_args</td>\n",
       "      <td>[-f, /Users/chris/Library/Jupyter/runtime/kernel-f7dc3fb7-1360-44e1-9eeb-14cb1e48f034.json]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>env.num_ip_addrs</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>env.max_workers</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>env.debugging</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>env.msg_level</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>env.msg_format</td>\n",
       "      <td>%(message)s</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>env.date_format</td>\n",
       "      <td>[%m.%d %H:%M:%S]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>env.output_home</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>env.logging_file</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>env.argument_file</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>time.started</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>time.settled</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>time.elapsed</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Create Llama 2 and Llama 3 helper functions - for chatbot type of apps, we'll use Llama 3 instruct and Llama 2 chat models, not the base models.",
   "id": "e60f5e428045f58b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-20T17:03:56.380425Z",
     "start_time": "2024-06-20T17:03:56.377345Z"
    }
   },
   "cell_type": "code",
   "source": [
    "GROQ_API_KEY = read_or(first_path_or(\"groq-tokens*\")) or getpass()\n",
    "REPLICATE_API_TOKEN = read_or(first_path_or(\"replicate-tokens*\")) or getpass()\n",
    "os.environ[\"GROQ_API_KEY\"] = GROQ_API_KEY\n",
    "os.environ[\"REPLICATE_API_TOKEN\"] = REPLICATE_API_TOKEN\n",
    "print(f'GROQ_API_KEY = {mask_str(os.environ.get(\"GROQ_API_KEY\"), start=4, end=-4)}')\n",
    "print(f'REPLICATE_API_TOKEN = {mask_str(os.environ.get(\"REPLICATE_API_TOKEN\"), start=3, end=-3)}')"
   ],
   "id": "2fb3426aaea6743b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GROQ_API_KEY = gsk_************************************************2Uuw\n",
      "REPLICATE_API_TOKEN = r8_**********************************MYA\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-20T17:03:56.473044Z",
     "start_time": "2024-06-20T17:03:56.381091Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import groq\n",
    "import replicate\n",
    "\n",
    "groq_client = groq.Groq(api_key=os.environ.get(\"GROQ_API_KEY\"))\n",
    "replicate_client = replicate.client.Client(api_token=os.environ.get(\"REPLICATE_API_TOKEN\"))\n",
    "\n",
    "\n",
    "def llm_chat_by_replicate(prompt, model):\n",
    "    output = replicate_client.run(\n",
    "        ref=model,\n",
    "        input={\"prompt\": prompt}\n",
    "    )\n",
    "    return ''.join(output)\n",
    "\n",
    "\n",
    "def llm_chat_by_groq(prompt, model):\n",
    "    chat_completion = groq_client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "    )\n",
    "    return chat_completion.choices[0].message.content\n",
    "\n",
    "\n",
    "chat_functions = {\n",
    "    \"llama2_7b_by_replicate\": lambda x: llm_chat_by_replicate(x, model=\"meta/llama-2-7b-chat\"),\n",
    "    \"llama2_70b_by_replicate\": lambda x: llm_chat_by_replicate(x, model=\"meta/llama-2-70b-chat\"),\n",
    "    \"llama3_8b_by_replicate\": lambda x: llm_chat_by_replicate(x, model=\"meta/meta-llama-3-8b-instruct\"),\n",
    "    \"llama3_70b_by_replicate\": lambda x: llm_chat_by_replicate(x, model=\"meta/meta-llama-3-70b-instruct\"),\n",
    "    # \"llama3_8b_by_groq\": lambda x: llm_chat_by_groq(x, model=\"llama3-8b-8192\"),\n",
    "    # \"llama3_70b_by_groq\": lambda x: llm_chat_by_groq(x, model=\"llama3-70b-8192\"),\n",
    "}\n",
    "chat_functions"
   ],
   "id": "dff82441129a04d1",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'llama2_7b_by_replicate': <function __main__.<lambda>(x)>,\n",
       " 'llama2_70b_by_replicate': <function __main__.<lambda>(x)>,\n",
       " 'llama3_8b_by_replicate': <function __main__.<lambda>(x)>,\n",
       " 'llama3_70b_by_replicate': <function __main__.<lambda>(x)>}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-20T17:03:56.669629Z",
     "start_time": "2024-06-20T17:03:56.473683Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain.document_loaders import WebBaseLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "\n",
    "def load_external_data(web_path):\n",
    "    # Step 1: Load the document from a web url\n",
    "    loader = WebBaseLoader(web_path)\n",
    "    documents = loader.load()\n",
    "\n",
    "    # Step 2: Split the document into chunks with a specified chunk size\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
    "    all_splits = text_splitter.split_documents(documents)\n",
    "\n",
    "    # Step 3: Store the document into a vector store with a specific embedding model\n",
    "    vectorstore = FAISS.from_documents(all_splits, HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\"))\n",
    "    return vectorstore.as_retriever()\n",
    "\n",
    "\n",
    "def llm_chat_with_retrieval(prompt, retriever, model, history=()):\n",
    "    llm = ChatGroq(temperature=0, model_name=model)\n",
    "    ret_chain = ConversationalRetrievalChain.from_llm(llm,\n",
    "                                                      retriever,\n",
    "                                                      return_source_documents=True)\n",
    "    output = ret_chain.invoke({\"question\": prompt, \"chat_history\": history})\n",
    "    return output[\"answer\"]\n",
    "\n",
    "\n",
    "langchain_functions = {\n",
    "    \"llama3_8b_by_langchain\": lambda p, r, h=(): llm_chat_with_retrieval(prompt=p, retriever=r, history=h, model=\"llama3-8b-8192\"),\n",
    "    \"llama3_70b_by_langchain\": lambda p, r, h=(): llm_chat_with_retrieval(prompt=p, retriever=r, history=h, model=\"llama3-70b-8192\"),\n",
    "}\n",
    "langchain_functions"
   ],
   "id": "c31645c1d6648671",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'llama3_8b_by_langchain': <function __main__.<lambda>(p, r, h=())>,\n",
       " 'llama3_70b_by_langchain': <function __main__.<lambda>(p, r, h=())>}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### **2.3 - Basic QA with Llama 2 and 3**",
   "id": "6b3e0c0e9cf2a4a1"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-20T17:04:04.470265Z",
     "start_time": "2024-06-20T17:03:56.671620Z"
    }
   },
   "cell_type": "code",
   "source": [
    "chat_prompt = \"The typical color of a llama is: \"\n",
    "outputs = dict()\n",
    "with JobTimer(\"2.3 - Basic QA with Llama 2 and 3 (1)\", rt=1, rb=1, rw=114, rc='=', verbose=1):\n",
    "    for name, function in chat_functions.items():\n",
    "        outputs[name] = function(chat_prompt)\n",
    "mds(outputs)"
   ],
   "id": "f7f3fe011bb650",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================================================================================\n",
      "[INIT] 2.3 - Basic QA with Llama 2 and 3 (1)\n",
      "==================================================================================================================\n",
      "HTTP Request: POST https://api.replicate.com/v1/models/meta/llama-2-7b-chat/predictions \"HTTP/1.1 201 Created\"\n",
      "HTTP Request: GET https://api.replicate.com/v1/predictions/c798hg6z05rgm0cg6td9an9tbc \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.replicate.com/v1/models/meta/llama-2-70b-chat/predictions \"HTTP/1.1 201 Created\"\n",
      "HTTP Request: GET https://api.replicate.com/v1/predictions/q0a819z341rgm0cg6td8kcz0z8 \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: GET https://api.replicate.com/v1/predictions/q0a819z341rgm0cg6td8kcz0z8 \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: GET https://api.replicate.com/v1/predictions/q0a819z341rgm0cg6td8kcz0z8 \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.replicate.com/v1/models/meta/meta-llama-3-8b-instruct/predictions \"HTTP/1.1 201 Created\"\n",
      "HTTP Request: GET https://api.replicate.com/v1/predictions/kq4kxvqcy1rgj0cg6td8mnzw44 \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.replicate.com/v1/models/meta/meta-llama-3-70b-instruct/predictions \"HTTP/1.1 201 Created\"\n",
      "HTTP Request: GET https://api.replicate.com/v1/predictions/8rvc23fgv1rj20cg6td8880hfm \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: GET https://api.replicate.com/v1/predictions/8rvc23fgv1rj20cg6td8880hfm \"HTTP/1.1 200 OK\"\n",
      "==================================================================================================================\n",
      "[EXIT] 2.3 - Basic QA with Llama 2 and 3 (1) ($=00:00:06.957)\n",
      "==================================================================================================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "<hr style='border:3px solid MidnightBlue; width:880px; padding:0; margin-left:0; margin-right:0; margin-top:10px; margin-bottom:15px;'>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ],
      "text/markdown": "**[llama2_7b_by_replicate]**\n>  Great, I'm glad you're interested in learning about llamas! The typical color of a llama is actually gray. Llamas are known for their distinctive gray coats, which can range in shade from a light gray to a darker, more mottled gray. Some llamas may also have white or yellow markings on their faces or bodies, but overall, their typical color is gray. Is there anything else you'd like to know about llamas?"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "<hr style='border:3px solid MidnightBlue; width:880px; padding:0; margin-left:0; margin-right:0; margin-top:10px; margin-bottom:15px;'>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ],
      "text/markdown": "**[llama2_70b_by_replicate]**\n>  The typical color of a llama is not a specific color, as llamas can come in a variety of colors and patterns. Some common colors include white, black, brown, gray, and various shades of red and tan. Some llamas may also have spotted or striped patterns. It's important to note that the color of a llama can vary depending on the individual animal and breed. Is there anything else I can help you with?"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "<hr style='border:3px solid MidnightBlue; width:880px; padding:0; margin-left:0; margin-right:0; margin-top:10px; margin-bottom:15px;'>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ],
      "text/markdown": "**[llama3_8b_by_replicate]**\n> \n> \n> The typical color of a llama is white, but they can also come in a variety of other colors such as light brown, dark brown, gray, black, and even spotted or patchy patterns!"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "<hr style='border:3px solid MidnightBlue; width:880px; padding:0; margin-left:0; margin-right:0; margin-top:10px; margin-bottom:15px;'>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ],
      "text/markdown": "**[llama3_70b_by_replicate]**\n> \n> \n> The typical color of a llama can vary, but they are often found in shades of:\n> \n> * White\n> * Brown\n> * Gray\n> * Black\n> * Roan (a mix of white and dark hairs)\n> * Fawn (a reddish-brown color)\n> \n> Some llamas can also have unique markings, such as spots or patches, on their coats."
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "<hr style='border:3px solid MidnightBlue; width:880px; padding:0; margin-left:0; margin-right:0; margin-top:10px; margin-bottom:15px;'>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-20T17:04:09.348390Z",
     "start_time": "2024-06-20T17:04:04.470849Z"
    }
   },
   "cell_type": "code",
   "source": [
    "chat_prompt = \"The typical color of a llama is what? Answer in one word.\"\n",
    "outputs = dict()\n",
    "with JobTimer(\"2.3 - Basic QA with Llama 2 and 3 (2)\", rt=1, rb=1, rw=114, rc='=', verbose=1):\n",
    "    for name, function in chat_functions.items():\n",
    "        outputs[name] = function(chat_prompt)\n",
    "mds(outputs)"
   ],
   "id": "ee8dac91906817bf",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================================================================================\n",
      "[INIT] 2.3 - Basic QA with Llama 2 and 3 (2)\n",
      "==================================================================================================================\n",
      "HTTP Request: POST https://api.replicate.com/v1/models/meta/llama-2-7b-chat/predictions \"HTTP/1.1 201 Created\"\n",
      "HTTP Request: GET https://api.replicate.com/v1/predictions/tamm5ffvv1rgj0cg6td83eqe60 \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.replicate.com/v1/models/meta/llama-2-70b-chat/predictions \"HTTP/1.1 201 Created\"\n",
      "HTTP Request: GET https://api.replicate.com/v1/predictions/rf2r5f7zvxrgp0cg6tdamsns10 \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.replicate.com/v1/models/meta/meta-llama-3-8b-instruct/predictions \"HTTP/1.1 201 Created\"\n",
      "HTTP Request: GET https://api.replicate.com/v1/predictions/0rqvjpg3tdrgg0cg6tdv3vhwa4 \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.replicate.com/v1/models/meta/meta-llama-3-70b-instruct/predictions \"HTTP/1.1 201 Created\"\n",
      "HTTP Request: GET https://api.replicate.com/v1/predictions/bjmb9eg7tdrj60cg6tds3929k4 \"HTTP/1.1 200 OK\"\n",
      "==================================================================================================================\n",
      "[EXIT] 2.3 - Basic QA with Llama 2 and 3 (2) ($=00:00:04.036)\n",
      "==================================================================================================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "<hr style='border:3px solid MidnightBlue; width:880px; padding:0; margin-left:0; margin-right:0; margin-top:10px; margin-bottom:15px;'>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ],
      "text/markdown": "**[llama2_7b_by_replicate]**\n>  Sure, I'd be happy to help! The typical color of a llama is gray."
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "<hr style='border:3px solid MidnightBlue; width:880px; padding:0; margin-left:0; margin-right:0; margin-top:10px; margin-bottom:15px;'>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ],
      "text/markdown": "**[llama2_70b_by_replicate]**\n>  Brown"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "<hr style='border:3px solid MidnightBlue; width:880px; padding:0; margin-left:0; margin-right:0; margin-top:10px; margin-bottom:15px;'>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ],
      "text/markdown": "**[llama3_8b_by_replicate]**\n> \n> \n> Brown."
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "<hr style='border:3px solid MidnightBlue; width:880px; padding:0; margin-left:0; margin-right:0; margin-top:10px; margin-bottom:15px;'>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ],
      "text/markdown": "**[llama3_70b_by_replicate]**\n> \n> \n> Beige."
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "<hr style='border:3px solid MidnightBlue; width:880px; padding:0; margin-left:0; margin-right:0; margin-top:10px; margin-bottom:15px;'>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## **3 - Chat conversation**",
   "id": "7cafbf05e3b4c9c0"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### **3.1 - Single-turn chat**",
   "id": "f712d948378b5c14"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-20T17:04:14.189468Z",
     "start_time": "2024-06-20T17:04:09.349074Z"
    }
   },
   "cell_type": "code",
   "source": [
    "chat_prompt = \"What is the average lifespan of a Llama? Answer the question in few words.\"\n",
    "outputs = dict()\n",
    "with JobTimer(\"3.1 - Single-turn chat (1)\", rt=1, rb=1, rw=114, rc='=', verbose=1):\n",
    "    for name, function in chat_functions.items():\n",
    "        outputs[name] = function(chat_prompt)\n",
    "mds(outputs)"
   ],
   "id": "8722520ca80990d6",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================================================================================\n",
      "[INIT] 3.1 - Single-turn chat (1)\n",
      "==================================================================================================================\n",
      "HTTP Request: POST https://api.replicate.com/v1/models/meta/llama-2-7b-chat/predictions \"HTTP/1.1 201 Created\"\n",
      "HTTP Request: GET https://api.replicate.com/v1/predictions/5hpkbxgexhrgg0cg6tdsq5bb3g \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.replicate.com/v1/models/meta/llama-2-70b-chat/predictions \"HTTP/1.1 201 Created\"\n",
      "HTTP Request: GET https://api.replicate.com/v1/predictions/pqpeb28jw9rgm0cg6tdvph3rar \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.replicate.com/v1/models/meta/meta-llama-3-8b-instruct/predictions \"HTTP/1.1 201 Created\"\n",
      "HTTP Request: GET https://api.replicate.com/v1/predictions/xgga8p0pwnrgg0cg6tdtwzzkvg \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.replicate.com/v1/models/meta/meta-llama-3-70b-instruct/predictions \"HTTP/1.1 201 Created\"\n",
      "HTTP Request: GET https://api.replicate.com/v1/predictions/fd6ty9rtrdrj00cg6tdrxt98br \"HTTP/1.1 200 OK\"\n",
      "==================================================================================================================\n",
      "[EXIT] 3.1 - Single-turn chat (1) ($=00:00:03.989)\n",
      "==================================================================================================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "<hr style='border:3px solid MidnightBlue; width:880px; padding:0; margin-left:0; margin-right:0; margin-top:10px; margin-bottom:15px;'>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ],
      "text/markdown": "**[llama2_7b_by_replicate]**\n>  The average lifespan of a llama is around 15-20 years."
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "<hr style='border:3px solid MidnightBlue; width:880px; padding:0; margin-left:0; margin-right:0; margin-top:10px; margin-bottom:15px;'>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ],
      "text/markdown": "**[llama2_70b_by_replicate]**\n>  The average lifespan of a llama is 20-30 years."
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "<hr style='border:3px solid MidnightBlue; width:880px; padding:0; margin-left:0; margin-right:0; margin-top:10px; margin-bottom:15px;'>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ],
      "text/markdown": "**[llama3_8b_by_replicate]**\n> \n> \n> 15-20 years on average."
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "<hr style='border:3px solid MidnightBlue; width:880px; padding:0; margin-left:0; margin-right:0; margin-top:10px; margin-bottom:15px;'>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ],
      "text/markdown": "**[llama3_70b_by_replicate]**\n> \n> \n> 20-30 years."
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "<hr style='border:3px solid MidnightBlue; width:880px; padding:0; margin-left:0; margin-right:0; margin-top:10px; margin-bottom:15px;'>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-20T17:04:19.765635Z",
     "start_time": "2024-06-20T17:04:14.190802Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# example without previous context. LLM's are stateless and cannot understand \"they\" without previous context\n",
    "chat_prompt = \"What animal family are they? Answer the question in few words.\"\n",
    "outputs = dict()\n",
    "with JobTimer(\"3.1 - Single-turn chat (2)\", rt=1, rb=1, rw=114, rc='=', verbose=1):\n",
    "    for name, function in chat_functions.items():\n",
    "        outputs[name] = function(chat_prompt)\n",
    "mds(outputs)"
   ],
   "id": "554690216a856f7c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================================================================================\n",
      "[INIT] 3.1 - Single-turn chat (2)\n",
      "==================================================================================================================\n",
      "HTTP Request: POST https://api.replicate.com/v1/models/meta/llama-2-7b-chat/predictions \"HTTP/1.1 201 Created\"\n",
      "HTTP Request: GET https://api.replicate.com/v1/predictions/36dgty91v9rgm0cg6tdvntts04 \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.replicate.com/v1/models/meta/llama-2-70b-chat/predictions \"HTTP/1.1 201 Created\"\n",
      "HTTP Request: GET https://api.replicate.com/v1/predictions/881ph4h5vdrgg0cg6tdsaw53fm \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.replicate.com/v1/models/meta/meta-llama-3-8b-instruct/predictions \"HTTP/1.1 201 Created\"\n",
      "HTTP Request: GET https://api.replicate.com/v1/predictions/7xda9ns9wnrgg0cg6tdvnsjz3m \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.replicate.com/v1/models/meta/meta-llama-3-70b-instruct/predictions \"HTTP/1.1 201 Created\"\n",
      "HTTP Request: GET https://api.replicate.com/v1/predictions/jzdsj1sds1rj00cg6tdtj2v3tg \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: GET https://api.replicate.com/v1/predictions/jzdsj1sds1rj00cg6tdtj2v3tg \"HTTP/1.1 200 OK\"\n",
      "==================================================================================================================\n",
      "[EXIT] 3.1 - Single-turn chat (2) ($=00:00:04.729)\n",
      "==================================================================================================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "<hr style='border:3px solid MidnightBlue; width:880px; padding:0; margin-left:0; margin-right:0; margin-top:10px; margin-bottom:15px;'>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ],
      "text/markdown": "**[llama2_7b_by_replicate]**\n>  Of course, I'd be happy to help! The animal family that the question is referring to is... (pausing for a moment to ensure the answer is accurate) ...the Canidae family, which includes dogs, wolves, and foxes."
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "<hr style='border:3px solid MidnightBlue; width:880px; padding:0; margin-left:0; margin-right:0; margin-top:10px; margin-bottom:15px;'>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ],
      "text/markdown": "**[llama2_70b_by_replicate]**\n>  They are primates."
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "<hr style='border:3px solid MidnightBlue; width:880px; padding:0; margin-left:0; margin-right:0; margin-top:10px; margin-bottom:15px;'>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ],
      "text/markdown": "**[llama3_8b_by_replicate]**\n> \n> \n> The animal family is Canidae."
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "<hr style='border:3px solid MidnightBlue; width:880px; padding:0; margin-left:0; margin-right:0; margin-top:10px; margin-bottom:15px;'>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ],
      "text/markdown": "**[llama3_70b_by_replicate]**\n> \n> \n> I'm happy to help! However, I need more context. Who are \"they\" referring to? Could you please provide more information or clarify your question?"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "<hr style='border:3px solid MidnightBlue; width:880px; padding:0; margin-left:0; margin-right:0; margin-top:10px; margin-bottom:15px;'>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "**Note: Llama 3 70b doesn't hallucinate.**",
   "id": "98e4863c5dd3a587"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### **3.2 - Multi-turn chat**\n",
    "Chat app requires us to send in previous context to LLM to get in valid responses. Below is an example of Multi-turn chat."
   ],
   "id": "cd8fb3eef8d283a1"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-20T17:04:25.584334Z",
     "start_time": "2024-06-20T17:04:19.766506Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# example of multi-turn chat, with storing previous context\n",
    "chat_prompt = \"\"\"\n",
    "User: What is the average lifespan of a Llama?\n",
    "Assistant: 15-20 years.\n",
    "User: What animal family are they?\n",
    "\"\"\"\n",
    "outputs = dict()\n",
    "with JobTimer(\"3.2 - Multi-turn chat (1)\", rt=1, rb=1, rw=114, rc='=', verbose=1):\n",
    "    for name, function in chat_functions.items():\n",
    "        outputs[name] = function(chat_prompt)\n",
    "mds(outputs)"
   ],
   "id": "dc83a7bc22016551",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================================================================================\n",
      "[INIT] 3.2 - Multi-turn chat (1)\n",
      "==================================================================================================================\n",
      "HTTP Request: POST https://api.replicate.com/v1/models/meta/llama-2-7b-chat/predictions \"HTTP/1.1 201 Created\"\n",
      "HTTP Request: GET https://api.replicate.com/v1/predictions/g93q9m1qm1rgp0cg6tds86x8k4 \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.replicate.com/v1/models/meta/llama-2-70b-chat/predictions \"HTTP/1.1 201 Created\"\n",
      "HTTP Request: GET https://api.replicate.com/v1/predictions/gpszsm1vjsrgj0cg6tdsbjz2zr \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.replicate.com/v1/models/meta/meta-llama-3-8b-instruct/predictions \"HTTP/1.1 201 Created\"\n",
      "HTTP Request: GET https://api.replicate.com/v1/predictions/c6txp0szjhrgp0cg6tdswdn9p0 \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.replicate.com/v1/models/meta/meta-llama-3-70b-instruct/predictions \"HTTP/1.1 201 Created\"\n",
      "HTTP Request: GET https://api.replicate.com/v1/predictions/q99bset3cxrj40cg6tdsxc7yzc \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: GET https://api.replicate.com/v1/predictions/q99bset3cxrj40cg6tdsxc7yzc \"HTTP/1.1 200 OK\"\n",
      "==================================================================================================================\n",
      "[EXIT] 3.2 - Multi-turn chat (1) ($=00:00:04.975)\n",
      "==================================================================================================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "<hr style='border:3px solid MidnightBlue; width:880px; padding:0; margin-left:0; margin-right:0; margin-top:10px; margin-bottom:15px;'>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ],
      "text/markdown": "**[llama2_7b_by_replicate]**\n>  Assistant: Thank you for your question! Llamas are actually not part of any specific animal family. They are members of the camel family (Camelidae) and are closely related to camels, alpacas, and vicuas. So, the answer to your question is: the camel family."
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "<hr style='border:3px solid MidnightBlue; width:880px; padding:0; margin-left:0; margin-right:0; margin-top:10px; margin-bottom:15px;'>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ],
      "text/markdown": "**[llama2_70b_by_replicate]**\n>  Assistant: Llamas are part of the Camelidae family, which includes camels and alpacas."
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "<hr style='border:3px solid MidnightBlue; width:880px; padding:0; margin-left:0; margin-right:0; margin-top:10px; margin-bottom:15px;'>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ],
      "text/markdown": "**[llama3_8b_by_replicate]**\n> \n> \n> Llamas belong to the camelid family, which also includes camels and alpacas."
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "<hr style='border:3px solid MidnightBlue; width:880px; padding:0; margin-left:0; margin-right:0; margin-top:10px; margin-bottom:15px;'>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ],
      "text/markdown": "**[llama3_70b_by_replicate]**\n> \n> \n> Llamas are part of the camelid family, which also includes camels, alpacas, guanacos, and vicuas."
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "<hr style='border:3px solid MidnightBlue; width:880px; padding:0; margin-left:0; margin-right:0; margin-top:10px; margin-bottom:15px;'>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "**Note: Llama 2 and 3 both behave well for using the chat history for follow up questions.**",
   "id": "fee4dc80f4437c2b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-20T17:04:30.412962Z",
     "start_time": "2024-06-20T17:04:25.584967Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# example of multi-turn chat, with storing previous context\n",
    "chat_prompt = \"\"\"\n",
    "User: What is the average lifespan of a Llama?\n",
    "Assistant: Sure! The average lifespan of a llama is around 20-30 years.\n",
    "User: What animal family are they?\n",
    "\n",
    "Answer the question with one word.\n",
    "\"\"\"\n",
    "outputs = dict()\n",
    "with JobTimer(\"3.2 - Multi-turn chat (2)\", rt=1, rb=1, rw=114, rc='=', verbose=1):\n",
    "    for name, function in chat_functions.items():\n",
    "        outputs[name] = function(chat_prompt)\n",
    "mds(outputs)"
   ],
   "id": "eaa41920ac7411b4",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================================================================================\n",
      "[INIT] 3.2 - Multi-turn chat (2)\n",
      "==================================================================================================================\n",
      "HTTP Request: POST https://api.replicate.com/v1/models/meta/llama-2-7b-chat/predictions \"HTTP/1.1 201 Created\"\n",
      "HTTP Request: GET https://api.replicate.com/v1/predictions/54z7r0teaxrgg0cg6tdvfn1yrw \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.replicate.com/v1/models/meta/llama-2-70b-chat/predictions \"HTTP/1.1 201 Created\"\n",
      "HTTP Request: GET https://api.replicate.com/v1/predictions/0t9dwhtja5rgj0cg6tdtvhskjg \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.replicate.com/v1/models/meta/meta-llama-3-8b-instruct/predictions \"HTTP/1.1 201 Created\"\n",
      "HTTP Request: GET https://api.replicate.com/v1/predictions/va3chvjp9xrgp0cg6tdt2w60j8 \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.replicate.com/v1/models/meta/meta-llama-3-70b-instruct/predictions \"HTTP/1.1 201 Created\"\n",
      "HTTP Request: GET https://api.replicate.com/v1/predictions/zn2bp52t65rj40cg6tdtkat854 \"HTTP/1.1 200 OK\"\n",
      "==================================================================================================================\n",
      "[EXIT] 3.2 - Multi-turn chat (2) ($=00:00:03.986)\n",
      "==================================================================================================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "<hr style='border:3px solid MidnightBlue; width:880px; padding:0; margin-left:0; margin-right:0; margin-top:10px; margin-bottom:15px;'>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ],
      "text/markdown": "**[llama2_7b_by_replicate]**\n>  Sure! The animal family that llamas belong to is: Camelidae."
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "<hr style='border:3px solid MidnightBlue; width:880px; padding:0; margin-left:0; margin-right:0; margin-top:10px; margin-bottom:15px;'>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ],
      "text/markdown": "**[llama2_70b_by_replicate]**\n>  Sure!\n> Camelids"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "<hr style='border:3px solid MidnightBlue; width:880px; padding:0; margin-left:0; margin-right:0; margin-top:10px; margin-bottom:15px;'>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ],
      "text/markdown": "**[llama3_8b_by_replicate]**\n> \n> \n> Camelid."
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "<hr style='border:3px solid MidnightBlue; width:880px; padding:0; margin-left:0; margin-right:0; margin-top:10px; margin-bottom:15px;'>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ],
      "text/markdown": "**[llama3_70b_by_replicate]**\n> \n> \n> Camelid"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "<hr style='border:3px solid MidnightBlue; width:880px; padding:0; margin-left:0; margin-right:0; margin-top:10px; margin-bottom:15px;'>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "**Both Llama 3 8b and Llama 2 70b follows instructions (e.g. \"Answer the question with one word\") better than Llama 2 7b in multi-turn chat.**",
   "id": "73a04476c5892b97"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### **3.3 - Prompt Engineering**\n",
    "* Prompt engineering refers to the science of designing effective prompts to get desired responses\n",
    "* Helps reduce hallucination"
   ],
   "id": "5ae6ff4bba0e4d4"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### **3.3.1 - In-Context Learning (e.g. Zero-shot, Few-shot)**\n",
    " * In-context learning - specific method of prompt engineering where demonstration of task are provided as part of prompt.\n",
    "  1. Zero-shot learning - model is performing tasks without any\n",
    "input examples.\n",
    "  2. Few or N-Shot Learning - model is performing and behaving based on input examples in user's prompt."
   ],
   "id": "195ccff05618fb37"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-20T17:04:35.259910Z",
     "start_time": "2024-06-20T17:04:30.413931Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Zero-shot example. To get positive/negative/neutral sentiment, we need to give examples in the prompt\n",
    "chat_prompt = '''\n",
    "Classify: I saw a Gecko.\n",
    "Sentiment: ?\n",
    "\n",
    "Give one word response.\n",
    "'''\n",
    "outputs = dict()\n",
    "with JobTimer(\"3.3.1 - In-Context Learning (1)\", rt=1, rb=1, rw=114, rc='=', verbose=1):\n",
    "    for name, function in chat_functions.items():\n",
    "        outputs[name] = function(chat_prompt)\n",
    "mds(outputs)"
   ],
   "id": "29b2d6ac41db8de",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================================================================================\n",
      "[INIT] 3.3.1 - In-Context Learning (1)\n",
      "==================================================================================================================\n",
      "HTTP Request: POST https://api.replicate.com/v1/models/meta/llama-2-7b-chat/predictions \"HTTP/1.1 201 Created\"\n",
      "HTTP Request: GET https://api.replicate.com/v1/predictions/qe24brb17nrgm0cg6tdr734raw \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.replicate.com/v1/models/meta/llama-2-70b-chat/predictions \"HTTP/1.1 201 Created\"\n",
      "HTTP Request: GET https://api.replicate.com/v1/predictions/2n1brvv56xrgj0cg6tdt7xj04g \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.replicate.com/v1/models/meta/meta-llama-3-8b-instruct/predictions \"HTTP/1.1 201 Created\"\n",
      "HTTP Request: GET https://api.replicate.com/v1/predictions/txb3xrb95srgg0cg6tdtg84ay8 \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.replicate.com/v1/models/meta/meta-llama-3-70b-instruct/predictions \"HTTP/1.1 201 Created\"\n",
      "HTTP Request: GET https://api.replicate.com/v1/predictions/6n5mgbkd19rj60cg6tdsrarv6w \"HTTP/1.1 200 OK\"\n",
      "==================================================================================================================\n",
      "[EXIT] 3.3.1 - In-Context Learning (1) ($=00:00:03.986)\n",
      "==================================================================================================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "<hr style='border:3px solid MidnightBlue; width:880px; padding:0; margin-left:0; margin-right:0; margin-top:10px; margin-bottom:15px;'>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ],
      "text/markdown": "**[llama2_7b_by_replicate]**\n>  Positive"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "<hr style='border:3px solid MidnightBlue; width:880px; padding:0; margin-left:0; margin-right:0; margin-top:10px; margin-bottom:15px;'>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ],
      "text/markdown": "**[llama2_70b_by_replicate]**\n>  Neutral"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "<hr style='border:3px solid MidnightBlue; width:880px; padding:0; margin-left:0; margin-right:0; margin-top:10px; margin-bottom:15px;'>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ],
      "text/markdown": "**[llama3_8b_by_replicate]**\n> \n> \n> Interesting"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "<hr style='border:3px solid MidnightBlue; width:880px; padding:0; margin-left:0; margin-right:0; margin-top:10px; margin-bottom:15px;'>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ],
      "text/markdown": "**[llama3_70b_by_replicate]**\n> \n> \n> Neutral"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "<hr style='border:3px solid MidnightBlue; width:880px; padding:0; margin-left:0; margin-right:0; margin-top:10px; margin-bottom:15px;'>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "**Note: Llama 3 has different opinions than Llama 2.**",
   "id": "4f32d037ab6fc4b6"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-20T17:04:40.453738Z",
     "start_time": "2024-06-20T17:04:35.261245Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# By giving examples to Llama, it understands the expected output format.\n",
    "chat_prompt = '''\n",
    "Classify: I love Llamas!\n",
    "Sentiment: Positive\n",
    "Classify: I dont like Snakes.\n",
    "Sentiment: Negative\n",
    "Classify: I saw a Gecko.\n",
    "Sentiment:\n",
    "\n",
    "Give one word response.\n",
    "'''\n",
    "outputs = dict()\n",
    "with JobTimer(\"3.3.1 - In-Context Learning (2)\", rt=1, rb=1, rw=114, rc='=', verbose=1):\n",
    "    for name, function in chat_functions.items():\n",
    "        outputs[name] = function(chat_prompt)\n",
    "mds(outputs)"
   ],
   "id": "82e85255a9c01c7",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================================================================================\n",
      "[INIT] 3.3.1 - In-Context Learning (2)\n",
      "==================================================================================================================\n",
      "HTTP Request: POST https://api.replicate.com/v1/models/meta/llama-2-7b-chat/predictions \"HTTP/1.1 201 Created\"\n",
      "HTTP Request: GET https://api.replicate.com/v1/predictions/p2wsw4vnc1rgj0cg6tdsrmzs0c \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.replicate.com/v1/models/meta/llama-2-70b-chat/predictions \"HTTP/1.1 201 Created\"\n",
      "HTTP Request: GET https://api.replicate.com/v1/predictions/qbptpckscxrgp0cg6tdvzwyd4w \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.replicate.com/v1/models/meta/meta-llama-3-8b-instruct/predictions \"HTTP/1.1 201 Created\"\n",
      "HTTP Request: GET https://api.replicate.com/v1/predictions/qjxvw5vxdxrgj0cg6tdvkwy80c \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.replicate.com/v1/models/meta/meta-llama-3-70b-instruct/predictions \"HTTP/1.1 201 Created\"\n",
      "HTTP Request: GET https://api.replicate.com/v1/predictions/007t9s41c9rj20cg6tdt631ner \"HTTP/1.1 200 OK\"\n",
      "==================================================================================================================\n",
      "[EXIT] 3.3.1 - In-Context Learning (2) ($=00:00:04.347)\n",
      "==================================================================================================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "<hr style='border:3px solid MidnightBlue; width:880px; padding:0; margin-left:0; margin-right:0; margin-top:10px; margin-bottom:15px;'>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ],
      "text/markdown": "**[llama2_7b_by_replicate]**\n>  Of course! I'm here to help. Here are the classifications and sentiments for the three statements you provided:\n> \n> Classify: I love Llamas!\n> Sentiment: Positive\n> \n> Classify: I don't like Snakes.\n> Sentiment: Negative\n> \n> Classify: I saw a Gecko.\n> Sentiment: Neutral"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "<hr style='border:3px solid MidnightBlue; width:880px; padding:0; margin-left:0; margin-right:0; margin-top:10px; margin-bottom:15px;'>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ],
      "text/markdown": "**[llama2_70b_by_replicate]**\n>  Neutral"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "<hr style='border:3px solid MidnightBlue; width:880px; padding:0; margin-left:0; margin-right:0; margin-top:10px; margin-bottom:15px;'>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ],
      "text/markdown": "**[llama3_8b_by_replicate]**\n> \n> \n> Neutral"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "<hr style='border:3px solid MidnightBlue; width:880px; padding:0; margin-left:0; margin-right:0; margin-top:10px; margin-bottom:15px;'>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ],
      "text/markdown": "**[llama3_70b_by_replicate]**\n> \n> \n> Neutral"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "<hr style='border:3px solid MidnightBlue; width:880px; padding:0; margin-left:0; margin-right:0; margin-top:10px; margin-bottom:15px;'>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "**Note: Llama 2, with few shots, has the same output \"Neutral\" as Llama 3, but Llama 2 doesn't follow instructions (Give one word response) well.**",
   "id": "9b26bbe1b025b17"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### **3.3.2 - Chain of Thought**\n",
    "\"Chain of thought\" enables complex reasoning through logical step by step thinking and generates meaningful and contextually relevant responses."
   ],
   "id": "ad3dfa53d543dbe1"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-20T17:04:45.304474Z",
     "start_time": "2024-06-20T17:04:40.454586Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Standard prompting\n",
    "chat_prompt = '''\n",
    "Llama started with 5 tennis balls. It buys 2 more cans of tennis balls. Each can has 3 tennis balls.\n",
    "How many tennis balls does Llama have?\n",
    "\n",
    "Answer in one word.\n",
    "'''\n",
    "outputs = dict()\n",
    "with JobTimer(\"3.3.2 - Chain of Thought (1)\", rt=1, rb=1, rw=114, rc='=', verbose=1):\n",
    "    for name, function in chat_functions.items():\n",
    "        outputs[name] = function(chat_prompt)\n",
    "mds(outputs)"
   ],
   "id": "58aac39e8c3e5920",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================================================================================\n",
      "[INIT] 3.3.2 - Chain of Thought (1)\n",
      "==================================================================================================================\n",
      "HTTP Request: POST https://api.replicate.com/v1/models/meta/llama-2-7b-chat/predictions \"HTTP/1.1 201 Created\"\n",
      "HTTP Request: GET https://api.replicate.com/v1/predictions/3m747jw8dsrgp0cg6tdsezjq84 \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.replicate.com/v1/models/meta/llama-2-70b-chat/predictions \"HTTP/1.1 201 Created\"\n",
      "HTTP Request: GET https://api.replicate.com/v1/predictions/arevx7ccdnrgj0cg6tdvz9hzqw \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.replicate.com/v1/models/meta/meta-llama-3-8b-instruct/predictions \"HTTP/1.1 201 Created\"\n",
      "HTTP Request: GET https://api.replicate.com/v1/predictions/7hdj73cgd9rgp0cg6tdvfnsygm \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.replicate.com/v1/models/meta/meta-llama-3-70b-instruct/predictions \"HTTP/1.1 201 Created\"\n",
      "HTTP Request: GET https://api.replicate.com/v1/predictions/g2ddq64m9hrj40cg6tdv1j8yfr \"HTTP/1.1 200 OK\"\n",
      "==================================================================================================================\n",
      "[EXIT] 3.3.2 - Chain of Thought (1) ($=00:00:04.010)\n",
      "==================================================================================================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "<hr style='border:3px solid MidnightBlue; width:880px; padding:0; margin-left:0; margin-right:0; margin-top:10px; margin-bottom:15px;'>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ],
      "text/markdown": "**[llama2_7b_by_replicate]**\n>  Sure, I'd be happy to help! Based on the information provided, Llama has a total of 7 tennis balls."
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "<hr style='border:3px solid MidnightBlue; width:880px; padding:0; margin-left:0; margin-right:0; margin-top:10px; margin-bottom:15px;'>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ],
      "text/markdown": "**[llama2_70b_by_replicate]**\n>  Sure! Here's my answer:\n> \n> Seven"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "<hr style='border:3px solid MidnightBlue; width:880px; padding:0; margin-left:0; margin-right:0; margin-top:10px; margin-bottom:15px;'>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ],
      "text/markdown": "**[llama3_8b_by_replicate]**\n> \n> \n> Seven."
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "<hr style='border:3px solid MidnightBlue; width:880px; padding:0; margin-left:0; margin-right:0; margin-top:10px; margin-bottom:15px;'>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ],
      "text/markdown": "**[llama3_70b_by_replicate]**\n> \n> \n> Eleven"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "<hr style='border:3px solid MidnightBlue; width:880px; padding:0; margin-left:0; margin-right:0; margin-top:10px; margin-bottom:15px;'>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "**Note: Llama 3-8b did not get the right answer because it was asked to answer in one word.**",
   "id": "84a3d62c402a3422"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-20T17:04:56.652587Z",
     "start_time": "2024-06-20T17:04:45.305381Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# By default, Llama 3 models follow \"Chain-Of-Thought\" prompting\n",
    "chat_prompt = '''\n",
    "Llama started with 5 tennis balls. It buys 2 more cans of tennis balls. Each can has 3 tennis balls.\n",
    "How many tennis balls does Llama have?\n",
    "'''\n",
    "outputs = dict()\n",
    "with JobTimer(\"3.3.2 - Chain of Thought (2)\", rt=1, rb=1, rw=114, rc='=', verbose=1):\n",
    "    for name, function in chat_functions.items():\n",
    "        outputs[name] = function(chat_prompt)\n",
    "mds(outputs)"
   ],
   "id": "16d7e505e95b773e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================================================================================\n",
      "[INIT] 3.3.2 - Chain of Thought (2)\n",
      "==================================================================================================================\n",
      "HTTP Request: POST https://api.replicate.com/v1/models/meta/llama-2-7b-chat/predictions \"HTTP/1.1 201 Created\"\n",
      "HTTP Request: GET https://api.replicate.com/v1/predictions/a72f3a4vd9rgg0cg6tdt0cfhtm \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: GET https://api.replicate.com/v1/predictions/a72f3a4vd9rgg0cg6tdt0cfhtm \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.replicate.com/v1/models/meta/llama-2-70b-chat/predictions \"HTTP/1.1 201 Created\"\n",
      "HTTP Request: GET https://api.replicate.com/v1/predictions/2mqtzq52bdrgj0cg6tdrqk0hv8 \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: GET https://api.replicate.com/v1/predictions/2mqtzq52bdrgj0cg6tdrqk0hv8 \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: GET https://api.replicate.com/v1/predictions/2mqtzq52bdrgj0cg6tdrqk0hv8 \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: GET https://api.replicate.com/v1/predictions/2mqtzq52bdrgj0cg6tdrqk0hv8 \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.replicate.com/v1/models/meta/meta-llama-3-8b-instruct/predictions \"HTTP/1.1 201 Created\"\n",
      "HTTP Request: GET https://api.replicate.com/v1/predictions/8q8ynynf41rgm0cg6tdve70c6w \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: GET https://api.replicate.com/v1/predictions/8q8ynynf41rgm0cg6tdve70c6w \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.replicate.com/v1/models/meta/meta-llama-3-70b-instruct/predictions \"HTTP/1.1 201 Created\"\n",
      "HTTP Request: GET https://api.replicate.com/v1/predictions/hzf0y4drahrj60cg6tdra611ac \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: GET https://api.replicate.com/v1/predictions/hzf0y4drahrj60cg6tdra611ac \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: GET https://api.replicate.com/v1/predictions/hzf0y4drahrj60cg6tdra611ac \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: GET https://api.replicate.com/v1/predictions/hzf0y4drahrj60cg6tdra611ac \"HTTP/1.1 200 OK\"\n",
      "==================================================================================================================\n",
      "[EXIT] 3.3.2 - Chain of Thought (2) ($=00:00:10.490)\n",
      "==================================================================================================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "<hr style='border:3px solid MidnightBlue; width:880px; padding:0; margin-left:0; margin-right:0; margin-top:10px; margin-bottom:15px;'>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ],
      "text/markdown": "**[llama2_7b_by_replicate]**\n>  Great question! I'm happy to help you with this puzzle.\n> \n> Let's break it down step by step:\n> \n> 1. Llama started with 5 tennis balls.\n> 2. Llama buys 2 more cans of tennis balls.\n> 3. Each can contains 3 tennis balls.\n> \n> So, if Llama buys 2 more cans of tennis balls, that means they will have:\n> \n> 5 + 2 x 3 = 19 tennis balls\n> \n> Therefore, Llama has 19 tennis balls in total! "
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "<hr style='border:3px solid MidnightBlue; width:880px; padding:0; margin-left:0; margin-right:0; margin-top:10px; margin-bottom:15px;'>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ],
      "text/markdown": "**[llama2_70b_by_replicate]**\n>  Sure, I'd be happy to help you with that!\n> \n> So, Llama started with 5 tennis balls, and then it bought 2 more cans of tennis balls. Each can has 3 tennis balls, so if we multiply 2 cans by 3 tennis balls per can, we get 6 tennis balls.\n> \n> Now, we need to add the 6 tennis balls that Llama just bought to the 5 tennis balls that Llama already had. That gives us a total of 11 tennis balls!\n> \n> So, to answer your question, Llama"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "<hr style='border:3px solid MidnightBlue; width:880px; padding:0; margin-left:0; margin-right:0; margin-top:10px; margin-bottom:15px;'>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ],
      "text/markdown": "**[llama3_8b_by_replicate]**\n> \n> \n> I'd be happy to help you with that!\n> \n> Llama started with 5 tennis balls. Then, it buys 2 more cans of tennis balls, and each can has 3 tennis balls. So, Llama buys a total of 2 x 3 = 6 tennis balls.\n> \n> Adding the 6 new tennis balls to the 5 Llama already had, Llama now has a total of:\n> \n> 5 + 6 = 11\n> \n> So, Llama has 11 tennis balls!"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "<hr style='border:3px solid MidnightBlue; width:880px; padding:0; margin-left:0; margin-right:0; margin-top:10px; margin-bottom:15px;'>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ],
      "text/markdown": "**[llama3_70b_by_replicate]**\n> \n> \n> Let's break it down step by step!\n> \n> Llama started with 5 tennis balls.\n> \n> It buys 2 more cans of tennis balls, and each can has 3 tennis balls. So, Llama gets 2 x 3 = 6 new tennis balls.\n> \n> To find the total number of tennis balls Llama has, we add the new tennis balls to the ones it already had: 5 (initial tennis balls) + 6 (new tennis balls) = 11\n> \n> Therefore, Llama now has 11 tennis balls!"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "<hr style='border:3px solid MidnightBlue; width:880px; padding:0; margin-left:0; margin-right:0; margin-top:10px; margin-bottom:15px;'>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "**Note: By default, Llama 3 models identify word problems and solves it step by step!**",
   "id": "4766b54fdc66fc97"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-20T17:05:10.400712Z",
     "start_time": "2024-06-20T17:04:56.653951Z"
    }
   },
   "cell_type": "code",
   "source": [
    "chat_prompt = \"\"\"\n",
    "15 of us want to go to a restaurant.\n",
    "Two of them have cars\n",
    "Each car can seat 5 people.\n",
    "Two of us have motorcycles.\n",
    "Each motorcycle can fit 2 people.\n",
    "Can we all get to the restaurant by car or motorcycle?\n",
    "Think step by step.\n",
    "Provide the answer as a single yes/no answer first.\n",
    "Then explain each intermediate step.\n",
    "\"\"\"\n",
    "outputs = dict()\n",
    "with JobTimer(\"3.3.2 - Chain of Thought (3)\", rt=1, rb=1, rw=114, rc='=', verbose=1):\n",
    "    for name, function in chat_functions.items():\n",
    "        outputs[name] = function(chat_prompt)\n",
    "mds(outputs)"
   ],
   "id": "8e775aa94222cd18",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================================================================================\n",
      "[INIT] 3.3.2 - Chain of Thought (3)\n",
      "==================================================================================================================\n",
      "HTTP Request: POST https://api.replicate.com/v1/models/meta/llama-2-7b-chat/predictions \"HTTP/1.1 201 Created\"\n",
      "HTTP Request: GET https://api.replicate.com/v1/predictions/0yjaeny7qsrgp0cg6tdv6jbxnm \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: GET https://api.replicate.com/v1/predictions/0yjaeny7qsrgp0cg6tdv6jbxnm \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.replicate.com/v1/models/meta/llama-2-70b-chat/predictions \"HTTP/1.1 201 Created\"\n",
      "HTTP Request: GET https://api.replicate.com/v1/predictions/18g5apeennrgp0cg6tdvqpy70g \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: GET https://api.replicate.com/v1/predictions/18g5apeennrgp0cg6tdvqpy70g \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: GET https://api.replicate.com/v1/predictions/18g5apeennrgp0cg6tdvqpy70g \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: GET https://api.replicate.com/v1/predictions/18g5apeennrgp0cg6tdvqpy70g \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.replicate.com/v1/models/meta/meta-llama-3-8b-instruct/predictions \"HTTP/1.1 201 Created\"\n",
      "HTTP Request: GET https://api.replicate.com/v1/predictions/9cd9sh6vhdrgp0cg6tdvq34j3m \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: GET https://api.replicate.com/v1/predictions/9cd9sh6vhdrgp0cg6tdvq34j3m \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: GET https://api.replicate.com/v1/predictions/9cd9sh6vhdrgp0cg6tdvq34j3m \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: GET https://api.replicate.com/v1/predictions/9cd9sh6vhdrgp0cg6tdvq34j3m \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: GET https://api.replicate.com/v1/predictions/9cd9sh6vhdrgp0cg6tdvq34j3m \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.replicate.com/v1/models/meta/meta-llama-3-70b-instruct/predictions \"HTTP/1.1 201 Created\"\n",
      "HTTP Request: GET https://api.replicate.com/v1/predictions/3fxddffb5xrj20cg6tdtybjke0 \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: GET https://api.replicate.com/v1/predictions/3fxddffb5xrj20cg6tdtybjke0 \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: GET https://api.replicate.com/v1/predictions/3fxddffb5xrj20cg6tdtybjke0 \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: GET https://api.replicate.com/v1/predictions/3fxddffb5xrj20cg6tdtybjke0 \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: GET https://api.replicate.com/v1/predictions/3fxddffb5xrj20cg6tdtybjke0 \"HTTP/1.1 200 OK\"\n",
      "==================================================================================================================\n",
      "[EXIT] 3.3.2 - Chain of Thought (3) ($=00:00:12.895)\n",
      "==================================================================================================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "<hr style='border:3px solid MidnightBlue; width:880px; padding:0; margin-left:0; margin-right:0; margin-top:10px; margin-bottom:15px;'>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ],
      "text/markdown": "**[llama2_7b_by_replicate]**\n>  Yes, it is possible for all 15 people to get to the restaurant by car or motorcycle. Here's how:\n> \n> Step 1: Two people with cars can transport 10 people (5 people per car) to the restaurant.\n> Step 2: Two people with motorcycles can transport 2 people (2 people per motorcycle) to the restaurant.\n> Step 3: The remaining 3 people can be transported by the two people with cars, or by the two motorcycles (if they are available).\n> \n> Therefore, the answer is yes, all 15 people can get to the restaurant by car or motorcycle."
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "<hr style='border:3px solid MidnightBlue; width:880px; padding:0; margin-left:0; margin-right:0; margin-top:10px; margin-bottom:15px;'>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ],
      "text/markdown": "**[llama2_70b_by_replicate]**\n>  No, we cannot all get to the restaurant by car or motorcycle.\n> \n> Let's break this down step by step:\n> \n> 1. Two people have cars.\n> 2. Each car can seat 5 people.\n> 3. Therefore, the two cars can seat a total of 10 people.\n> 4. Two people have motorcycles.\n> 5. Each motorcycle can fit 2 people.\n> 6. Therefore, the two motorcycles can fit a total of 4 people.\n> 7. In total, we have 15 people who want to go to the restaurant.\n> "
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "<hr style='border:3px solid MidnightBlue; width:880px; padding:0; margin-left:0; margin-right:0; margin-top:10px; margin-bottom:15px;'>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ],
      "text/markdown": "**[llama3_8b_by_replicate]**\n> \n> \n> **Yes**\n> \n> Here's the step-by-step explanation:\n> \n> 1. We have 15 people who want to go to the restaurant.\n> 2. Two people have cars, which can seat 5 people each. This means each car can take 5 people to the restaurant.\n> 3. So, we can take 2 x 5 = 10 people to the restaurant using the cars.\n> 4. We still have 15 - 10 = 5 people left who haven't been accounted for yet.\n> 5. Two people have motorcycles, which can seat 2 people each. This means each motorcycle can take 2 people to the restaurant.\n> 6. We can take 2 x 2 = 4 people to the restaurant using the motorcycles.\n> 7. We still have 5 - 4 = 1 person left who hasn't been accounted for yet.\n> 8. Since we only have 1 person left, we can accommodate them by pairing them with one of the people who already has a car or motorcycle.\n> 9. Therefore, we can take all 15 people to the restaurant using the available cars and motorcycles."
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "<hr style='border:3px solid MidnightBlue; width:880px; padding:0; margin-left:0; margin-right:0; margin-top:10px; margin-bottom:15px;'>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ],
      "text/markdown": "**[llama3_70b_by_replicate]**\n> \n> \n> **Answer:** NO\n> \n> Here's the step-by-step explanation:\n> \n> 1. We have 15 people who want to go to the restaurant.\n> 2. We have 2 cars, each of which can seat 5 people. This means we can transport a total of 2 x 5 = 10 people by car.\n> 3. We still have 15 - 10 = 5 people left who need transportation.\n> 4. We have 2 motorcycles, each of which can fit 2 people. This means we can transport a total of 2 x 2 = 4 people by motorcycle.\n> 5. We still have 5 - 4 = 1 person left who needs transportation, but we don't have any more vehicles to accommodate them.\n> \n> Therefore, we cannot transport all 15 people to the restaurant by car or motorcycle."
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "<hr style='border:3px solid MidnightBlue; width:880px; padding:0; margin-left:0; margin-right:0; margin-top:10px; margin-bottom:15px;'>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "**Note: Llama 3 70b model works correctly in this example.**",
   "id": "2a65a61a6e158af4"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "**Summary: Llama 2 often needs encouragement for step by step thinking to correctly reasoning. Llama 3 understands, reasons and explains better, making chain of thought unnecessary in the cases above.**",
   "id": "6934a03096c51964"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### **3.4 - Retrieval Augmented Generation (RAG)**\n",
    "* Prompt Eng Limitations - Knowledge cutoff & lack of specialized data\n",
    "* Retrieval Augmented Generation(RAG) allows us to retrieve snippets of information from external data sources and augment it to the user's prompt to get tailored responses from Llama 2.\n",
    "\n",
    "For our demo, we are going to download an external PDF file from a URL and query against the content in the pdf file to get contextually relevant information back with the help of Llama!"
   ],
   "id": "1905410ca49b6e36"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-20T17:05:10.405574Z",
     "start_time": "2024-06-20T17:05:10.402091Z"
    }
   },
   "cell_type": "code",
   "source": "rag_arch()",
   "id": "17bd3c2b24aa01cc",
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://mermaid.ink/img/CiAgICBmbG93Y2hhcnQgVEQKICAgICAgICBBW1VzZXIgUHJvbXB0c10gLS0+IEIoRnJhbWV3b3JrcyBlLmcuIExhbmdDaGFpbikKICAgICAgICBCIDwtLT4gfERhdGFiYXNlLCBEb2NzLCBYTFN8Q1tmYTpmYS1kYXRhYmFzZSBFeHRlcm5hbCBEYXRhXQogICAgICAgIEIgLS0+fEFQSXxEW0xsYW1hIDNdCiAgICAgICAgY2xhc3NEZWYgZGVmYXVsdCBmaWxsOiNDQ0U2RkYsc3Ryb2tlOiM4NEJDRjUsdGV4dENvbG9yOiMxQzJCMzMsZm9udEZhbWlseTp0cmVidWNoZXQgbXM7CiAgICA=\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 21
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### **3.4.1 - LangChain**\n",
    "LangChain is a framework that helps make it easier to implement RAG."
   ],
   "id": "e41075a070214735"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-20T17:05:16.310792Z",
     "start_time": "2024-06-20T17:05:10.406464Z"
    }
   },
   "cell_type": "code",
   "source": [
    "with JobTimer(\"3.4.1 - LangChain\", rt=1, rb=1, rw=114, rc='=', verbose=1):\n",
    "    about_llama3 = load_external_data([\"https://huggingface.co/blog/llama3\"])"
   ],
   "id": "2b2a7c0fd81869e3",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================================================================================\n",
      "[INIT] 3.4.1 - LangChain\n",
      "==================================================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chris/miniforge3/envs/LLM-based/lib/python3.11/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:11: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use pytorch device_name: mps\n",
      "Load pretrained SentenceTransformer: sentence-transformers/all-mpnet-base-v2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chris/miniforge3/envs/LLM-based/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading faiss.\n",
      "Successfully loaded faiss.\n",
      "==================================================================================================================\n",
      "[EXIT] 3.4.1 - LangChain ($=00:00:05.069)\n",
      "==================================================================================================================\n"
     ]
    }
   ],
   "execution_count": 22
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### **3.4.2 - LangChain Q&A Retriever**\n",
    "* ConversationalRetrievalChain\n",
    "* Query the Source documents"
   ],
   "id": "aeb97297c614ef7d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-20T17:05:20.125738Z",
     "start_time": "2024-06-20T17:05:16.312001Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# no chat history passed\n",
    "chat_history = {name: [] for name in langchain_functions.keys()}\n",
    "query = \"Whats new with Llama 3?\"\n",
    "outputs = dict()\n",
    "with JobTimer(\"3.4.2 - LangChain Q&A Retriever (1)\", rt=1, rb=1, rw=114, rc='=', verbose=1):\n",
    "    for name, function in langchain_functions.items():\n",
    "        outputs[name] = function(query, about_llama3)\n",
    "        chat_history[name].append((query, outputs[name]))\n",
    "mds(outputs)"
   ],
   "id": "eaee7e68097dd6f5",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================================================================================\n",
      "[INIT] 3.4.2 - LangChain Q&A Retriever (1)\n",
      "==================================================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "==================================================================================================================\n",
      "[EXIT] 3.4.2 - LangChain Q&A Retriever (1) ($=00:00:02.978)\n",
      "==================================================================================================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "<hr style='border:3px solid MidnightBlue; width:880px; padding:0; margin-left:0; margin-right:0; margin-top:10px; margin-bottom:15px;'>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ],
      "text/markdown": "**[llama3_8b_by_langchain]**\n> According to the provided context, the main changes in Llama 3 compared to Llama 2 are:\n> \n> 1. A new tokenizer that expands the vocabulary size to 128,256 tokens, allowing for more efficient encoding of text and potentially stronger multilingualism.\n> 2. The introduction of two model sizes: 8B for efficient deployment and development on consumer-size GPU, and 70B for large-scale AI native applications.\n> 3. The availability of base and instruction-tuned variants for each model size.\n> 4. The release of Llama Guard 2, a safety fine-tuned version of Llama 3 8B.\n> \n> These changes aim to improve the performance and versatility of the Llama 3 models, making them more suitable for a wider range of applications."
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "<hr style='border:3px solid MidnightBlue; width:880px; padding:0; margin-left:0; margin-right:0; margin-top:10px; margin-bottom:15px;'>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ],
      "text/markdown": "**[llama3_70b_by_langchain]**\n> According to the text, the new features of Llama 3 include:\n> \n> * 4 new open LLM models, with 2 sizes (8B and 70B parameters) and 2 variants (base and instruct-tuned)\n> * A new tokenizer that expands the vocabulary size to 128,256 (from 32K tokens in Llama 2)\n> * A larger vocabulary that can encode text more efficiently and potentially yield stronger multilingualism\n> * A larger embedding input and output matrices, which accounts for a good portion of the parameter count increase\n> * A new version of Llama Guard, fine-tuned on Llama 3 8B and released as Llama Guard 2 (safety fine-tune)"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "<hr style='border:3px solid MidnightBlue; width:880px; padding:0; margin-left:0; margin-right:0; margin-top:10px; margin-bottom:15px;'>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-20T17:05:26.481672Z",
     "start_time": "2024-06-20T17:05:20.126661Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# This time your previous question and answer will be included as a chat history which will enable the ability to ask follow up questions.\n",
    "query = \"What two sizes?\"\n",
    "outputs = dict()\n",
    "with JobTimer(\"3.4.2 - LangChain Q&A Retriever (2)\", rt=1, rb=1, rw=114, rc='=', verbose=1):\n",
    "    for name, function in langchain_functions.items():\n",
    "        outputs[name] = function(query, about_llama3, chat_history[name])\n",
    "        chat_history[name].append((query, outputs[name]))\n",
    "mds(outputs)"
   ],
   "id": "b91f299b532f001d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================================================================================\n",
      "[INIT] 3.4.2 - LangChain Q&A Retriever (2)\n",
      "==================================================================================================================\n",
      "HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "==================================================================================================================\n",
      "[EXIT] 3.4.2 - LangChain Q&A Retriever (2) ($=00:00:05.509)\n",
      "==================================================================================================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "<hr style='border:3px solid MidnightBlue; width:880px; padding:0; margin-left:0; margin-right:0; margin-top:10px; margin-bottom:15px;'>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ],
      "text/markdown": "**[llama3_8b_by_langchain]**\n> According to the context, the two model sizes introduced in Llama 3 are:\n> \n> 1. 8B\n> 2. 70B"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "<hr style='border:3px solid MidnightBlue; width:880px; padding:0; margin-left:0; margin-right:0; margin-top:10px; margin-bottom:15px;'>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ],
      "text/markdown": "**[llama3_70b_by_langchain]**\n> The two sizes of the new open LLM models in Llama 3 are 8B and 70B parameters."
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "<hr style='border:3px solid MidnightBlue; width:880px; padding:0; margin-left:0; margin-right:0; margin-top:10px; margin-bottom:15px;'>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 24
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## **4 - Fine-Tuning Models**\n",
    "* Limitatons of Prompt Eng and RAG\n",
    "* Fine-Tuning Arch\n",
    "* Types (PEFT, LoRA, QLoRA)\n",
    "* Using PyTorch for Pre-Training & Fine-Tuning\n",
    "* Evals + Quality\n",
    "\n",
    "* Examples of Fine-Tuning:\n",
    "  * [Meta Llama Recipes](https://github.com/meta-llama/llama-recipes/tree/main/recipes/finetuning)\n",
    "  * [Hugging Face fine-tuning with Llama 3](https://huggingface.co/blog/llama3#fine-tuning-with-%F0%9F%A4%97-trl)"
   ],
   "id": "485ed0b25f0baab6"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## **5 - Responsible AI**\n",
    "* Power + Responsibility\n",
    "* Hallucinations\n",
    "* Input & Output Safety\n",
    "* Red-teaming (simulating real-world cyber attackers)\n",
    "* [Responsible Use Guide](https://ai.meta.com/llama/responsible-use-guide/)"
   ],
   "id": "20d9ee189c8ef88d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## **6 - Conclusion**\n",
    "* Active research on LLMs and Llama\n",
    "* Leverage the power of Llama and its open community\n",
    "* Safety and responsible use is paramount!\n",
    "\n",
    "* Call-To-Action\n",
    "  * [Replicate Free Credits](https://replicate.fyi/connect2023) for Connect attendees!\n",
    "  * This notebook is available through Llama Github recipes\n",
    "  * Use Llama in your projects and give us feedback\n"
   ],
   "id": "8e65302a917a841b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### **Resources**\n",
    "- [Meta Llama 3 Blog](https://ai.meta.com/blog/meta-llama-3/)\n",
    "- [Getting Started with Meta Llama](https://llama.meta.com/docs/get-started)\n",
    "- [Llama 3 repo](https://github.com/meta-llama/llama3)\n",
    "- [Llama 3 model card](https://github.com/meta-llama/llama3/blob/main/MODEL_CARD.md)\n",
    "- [LLama 3 Recipes repo](https://github.com/meta-llama/llama-recipes)\n",
    "- [Responsible Use Guide](https://ai.meta.com/llama/responsible-use-guide/)\n",
    "- [Acceptable Use Policy](https://ai.meta.com/llama/use-policy/)\n"
   ],
   "id": "d201a9aa70cfcad3"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
