{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d680961880222a81",
   "metadata": {},
   "source": [
    "# **Getting to know Llama 2: Everything you need to start building**\n",
    "Our goal in this session is to provide a guided tour of Llama 2, including understanding different Llama 2 models, how and where to access them, Generative AI and Chatbot architectures, prompt engineering, RAG (Retrieval Augmented Generation), Fine-tuning and more. All this is implemented with a starter code for you to take it and use it in your Llama 2 projects."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c545e17b77f35ab",
   "metadata": {},
   "source": [
    "## **0 - Prerequisites**\n",
    "* Basic understanding of Large Language Models\n",
    "* Basic understanding of Python"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-30T11:34:32.130755Z",
     "start_time": "2024-04-30T11:34:32.124832Z"
    }
   },
   "cell_type": "code",
   "source": [
    "%%html\n",
    "<style>\n",
    "    .LLM {\n",
    "        background-color: skyblue;\n",
    "        font-size: 16pt;\n",
    "        padding: 10px;\n",
    "        margin: 10px;\n",
    "    }\n",
    "</style>"
   ],
   "id": "e22e289078070a39",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "<style>\n",
       "    .LLM {\n",
       "        background-color: skyblue;\n",
       "        font-size: 16pt;\n",
       "        padding: 10px;\n",
       "        margin: 10px;\n",
       "    }\n",
       "</style>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "id": "d87dc3a5a98e8f7f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-30T11:34:32.816100Z",
     "start_time": "2024-04-30T11:34:32.154041Z"
    }
   },
   "source": [
    "import base64\n",
    "from getpass import getpass\n",
    "\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, Image, Markdown, HTML\n",
    "\n",
    "from chrisbase.data import *\n",
    "from chrisbase.io import *\n",
    "from chrisbase.util import *\n",
    "\n",
    "\n",
    "def mm(graph):\n",
    "    graphbytes = graph.encode(\"ascii\")\n",
    "    base64_bytes = base64.b64encode(graphbytes)\n",
    "    base64_string = base64_bytes.decode(\"ascii\")\n",
    "    display(Image(url=\"https://mermaid.ink/img/\" + base64_string))\n",
    "\n",
    "\n",
    "def md(t):\n",
    "    display(Markdown(t))\n",
    "\n",
    "\n",
    "def html(t):\n",
    "    display(widgets.widgets.HTML(t))\n",
    "    # display(HTML(t))\n",
    "\n",
    "\n",
    "def genai_app_arch():\n",
    "    mm(\"\"\"\n",
    "    flowchart TD\n",
    "        A[Users] --> B(Applications e.g. mobile, web)\n",
    "        B --> |Hosted API|C(Platforms e.g. Custom, HuggingFace, Replicate)\n",
    "        B -- optional --> E(Frameworks e.g. LangChain)\n",
    "        C-->|User Input|D[Llama 2]\n",
    "        D-->|Model Output|C\n",
    "        E --> C\n",
    "        classDef default fill:#CCE6FF,stroke:#84BCF5,textColor:#1C2B33,fontFamily:trebuchet ms;\n",
    "    \"\"\")\n",
    "\n",
    "\n",
    "def rag_arch():\n",
    "    mm(\"\"\"\n",
    "    flowchart TD\n",
    "        A[User Prompts] --> B(Frameworks e.g. LangChain)\n",
    "        B <--> |Database, Docs, XLS|C[fa:fa-database External Data]\n",
    "        B -->|API|D[Llama 2]\n",
    "        classDef default fill:#CCE6FF,stroke:#84BCF5,textColor:#1C2B33,fontFamily:trebuchet ms;\n",
    "  \"\"\")\n",
    "\n",
    "\n",
    "def llama2_family():\n",
    "    mm(\"\"\"\n",
    "    graph LR;\n",
    "        llama-2 --> llama-2-7b\n",
    "        llama-2 --> llama-2-13b\n",
    "        llama-2 --> llama-2-70b\n",
    "        llama-2-7b --> llama-2-7b-chat\n",
    "        llama-2-13b --> llama-2-13b-chat\n",
    "        llama-2-70b --> llama-2-70b-chat\n",
    "        classDef default fill:#CCE6FF,stroke:#84BCF5,textColor:#1C2B33,fontFamily:trebuchet ms;\n",
    "    \"\"\")\n",
    "\n",
    "\n",
    "def apps_and_llms():\n",
    "    mm(\"\"\"\n",
    "    graph LR;\n",
    "        users --> apps\n",
    "        apps --> frameworks\n",
    "        frameworks --> platforms\n",
    "        platforms --> Llama 2\n",
    "        classDef default fill:#CCE6FF,stroke:#84BCF5,textColor:#1C2B33,fontFamily:trebuchet ms;\n",
    "    \"\"\")\n",
    "\n",
    "\n",
    "# Create a text widget\n",
    "API_KEY = widgets.Password(\n",
    "    value='',\n",
    "    placeholder='',\n",
    "    description='API_KEY:',\n",
    "    disabled=False\n",
    ")\n",
    "\n",
    "\n",
    "def bot_arch():\n",
    "    mm(\"\"\"\n",
    "    graph LR;\n",
    "        user --> prompt\n",
    "        prompt --> i_safety\n",
    "        i_safety --> context\n",
    "        context --> Llama_2\n",
    "        Llama_2 --> output\n",
    "        output --> o_safety\n",
    "        i_safety --> memory\n",
    "        o_safety --> memory\n",
    "        memory --> context\n",
    "        o_safety --> user\n",
    "        classDef default fill:#CCE6FF,stroke:#84BCF5,textColor:#1C2B33,fontFamily:trebuchet ms;\n",
    "    \"\"\")\n",
    "\n",
    "\n",
    "def fine_tuned_arch():\n",
    "    mm(\"\"\"\n",
    "    graph LR;\n",
    "        Custom_Dataset --> Pre-trained_Llama\n",
    "        Pre-trained_Llama --> Fine-tuned_Llama\n",
    "        Fine-tuned_Llama --> RLHF\n",
    "        RLHF --> |Loss:Cross-Entropy|Fine-tuned_Llama\n",
    "        classDef default fill:#CCE6FF,stroke:#84BCF5,textColor:#1C2B33,fontFamily:trebuchet ms;\n",
    "    \"\"\")\n",
    "\n",
    "\n",
    "def load_data_faiss_arch():\n",
    "    mm(\"\"\"\n",
    "    graph LR;\n",
    "        documents --> textsplitter\n",
    "        textsplitter --> embeddings\n",
    "        embeddings --> vectorstore\n",
    "        classDef default fill:#CCE6FF,stroke:#84BCF5,textColor:#1C2B33,fontFamily:trebuchet ms;\n",
    "    \"\"\")\n",
    "\n",
    "\n",
    "def mem_context():\n",
    "    mm(\"\"\"\n",
    "    graph LR\n",
    "        context(text)\n",
    "        user_prompt --> context\n",
    "        instruction --> context\n",
    "        examples --> context\n",
    "        memory --> context\n",
    "        context --> tokenizer\n",
    "        tokenizer --> embeddings\n",
    "        embeddings --> LLM\n",
    "        classDef default fill:#CCE6FF,stroke:#84BCF5,textColor:#1C2B33,fontFamily:trebuchet ms;\n",
    "    \"\"\")\n"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "id": "acd794f15ce1f743",
   "metadata": {},
   "source": [
    "## **1 - Understanding Llama 2**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "548692b6a6dd66c",
   "metadata": {},
   "source": [
    "### **1.1 - What is Llama 2?**\n",
    "* State of the art (SOTA), Open Source LLM\n",
    "* 7B, 13B, 70B\n",
    "* Pretrained + Chat\n",
    "* Choosing model: Size, Quality, Cost, Speed\n",
    "* [Research paper](https://ai.meta.com/research/publications/llama-2-open-foundation-and-fine-tuned-chat-models/)\n",
    "* [Responsible use guide](https://ai.meta.com/llama/responsible-use-guide/)"
   ]
  },
  {
   "cell_type": "code",
   "id": "f7d7c15448e3f170",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-30T11:34:32.819192Z",
     "start_time": "2024-04-30T11:34:32.817036Z"
    }
   },
   "source": [
    "llama2_family()"
   ],
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://mermaid.ink/img/CiAgICBncmFwaCBMUjsKICAgICAgICBsbGFtYS0yIC0tPiBsbGFtYS0yLTdiCiAgICAgICAgbGxhbWEtMiAtLT4gbGxhbWEtMi0xM2IKICAgICAgICBsbGFtYS0yIC0tPiBsbGFtYS0yLTcwYgogICAgICAgIGxsYW1hLTItN2IgLS0+IGxsYW1hLTItN2ItY2hhdAogICAgICAgIGxsYW1hLTItMTNiIC0tPiBsbGFtYS0yLTEzYi1jaGF0CiAgICAgICAgbGxhbWEtMi03MGIgLS0+IGxsYW1hLTItNzBiLWNoYXQKICAgICAgICBjbGFzc0RlZiBkZWZhdWx0IGZpbGw6I0NDRTZGRixzdHJva2U6Izg0QkNGNSx0ZXh0Q29sb3I6IzFDMkIzMyxmb250RmFtaWx5OnRyZWJ1Y2hldCBtczsKICAgIA==\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "id": "190b25e8cdc4c64d",
   "metadata": {},
   "source": [
    "### **1.2 - Accessing Llama 2**\n",
    "* Download + Self Host (on-premise)\n",
    "* Hosted API Platform (e.g. [Replicate](https://replicate.com/meta))\n",
    "* Hosted Container Platform (e.g. [Azure](https://techcommunity.microsoft.com/t5/ai-machine-learning-blog/introducing-llama-2-on-azure/ba-p/3881233), [AWS](https://aws.amazon.com/blogs/machine-learning/llama-2-foundation-models-from-meta-are-now-available-in-amazon-sagemaker-jumpstart/), [GCP](https://console.cloud.google.com/vertex-ai/publishers/google/model-garden/139))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7f0f1cf7c500be4",
   "metadata": {},
   "source": [
    "### **1.3 - Use Cases of Llama 2**\n",
    "* Content Generation\n",
    "* Chatbots\n",
    "* Summarization\n",
    "* Programming (e.g. Code Llama)\n",
    "* and many more..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "965614383680fb8",
   "metadata": {},
   "source": [
    "## **2 - Using Llama 2**\n",
    "In this notebook, we are going to access [Llama 13b chat model](https://replicate.com/meta/llama-2-13b-chat) using hosted API from Replicate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67f1a112c1342b73",
   "metadata": {},
   "source": [
    "### **2.1 - Install dependencies**"
   ]
  },
  {
   "cell_type": "code",
   "id": "8a77a8bb71e58ed5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-30T11:34:33.377654Z",
     "start_time": "2024-04-30T11:34:32.819764Z"
    }
   },
   "source": [
    "# Install dependencies and initialize\n",
    "!pip list | grep -E \"torch|transformers|chris|langchain|replicate|faiss|jupyter\""
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chrisbase                 0.5.2\r\n",
      "faiss-cpu                 1.8.0\r\n",
      "jupyter                   1.0.0\r\n",
      "jupyter_client            8.6.1\r\n",
      "jupyter-console           6.6.3\r\n",
      "jupyter_core              5.7.2\r\n",
      "jupyter-events            0.10.0\r\n",
      "jupyter-lsp               2.2.5\r\n",
      "jupyter_server            2.14.0\r\n",
      "jupyter_server_terminals  0.5.3\r\n",
      "jupyterlab                4.1.8\r\n",
      "jupyterlab_pygments       0.3.0\r\n",
      "jupyterlab_server         2.27.1\r\n",
      "jupyterlab_widgets        3.0.10\r\n",
      "langchain                 0.1.16\r\n",
      "langchain-community       0.0.34\r\n",
      "langchain-core            0.1.46\r\n",
      "langchain-text-splitters  0.0.1\r\n",
      "replicate                 0.25.2\r\n",
      "sentence-transformers     2.7.0\r\n",
      "torch                     2.3.0\r\n",
      "transformers              4.40.1\r\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "id": "8068c18c446acaec",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-30T11:34:33.382257Z",
     "start_time": "2024-04-30T11:34:33.380108Z"
    }
   },
   "source": [
    "# model url on Replicate platform that we will use for inferencing\n",
    "# We will use llama 2 13b chat model hosted on replicate server ()agent_name = \"llama-2-13b-chat\"\n",
    "model_ref = f\"meta/llama-2-13b-chat:f4e2de70d66816a838a89eeeb621910adffb0dd0baba3976c96980970978018d\""
   ],
   "outputs": [],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "id": "45ec8c200c5b6f57",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-30T11:34:33.401963Z",
     "start_time": "2024-04-30T11:34:33.382990Z"
    }
   },
   "source": [
    "# We will use Replicate hosted cloud environment\n",
    "# Obtain Replicate API key â†’ https://replicate.com/account/api-tokens)\n",
    "\n",
    "# enter your replicate api token\n",
    "REPLICATE_API_TOKEN = read_or(first_path_or(\".replicate*\")) or getpass()\n",
    "os.environ[\"REPLICATE_API_TOKEN\"] = REPLICATE_API_TOKEN\n",
    "\n",
    "logging.getLogger(\"IPKernelApp\").setLevel(logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "args = CommonArguments(\n",
    "    env=ProjectEnv(\n",
    "        project=\"LLM-based\",\n",
    "        job_name=\"Llama 2 13b Chat\",\n",
    "        msg_level=logging.INFO,\n",
    "        msg_format=LoggingFormat.BRIEF_00,\n",
    "    )\n",
    ")\n",
    "args.dataframe()"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "      CommonArguments                                                                                        value\n",
       "0                 tag                                                                                         None\n",
       "1         env.project                                                                                    LLM-based\n",
       "2        env.job_name                                                                             Llama 2 13b Chat\n",
       "3     env.job_version                                                                                         None\n",
       "4        env.hostname                                                                           ChrisBookPro.local\n",
       "5        env.hostaddr                                                                                  192.168.0.8\n",
       "6      env.time_stamp                                                                                  0430.203432\n",
       "7     env.python_path                                        /Users/chris/miniforge3/envs/LLM-based/bin/python3.11\n",
       "8     env.current_dir                                                                  /Users/chris/proj/LLM-based\n",
       "9    env.current_file                                    /Users/chris/proj/LLM-based/Getting_to_know_Llama_2.ipynb\n",
       "10    env.working_dir                                                                  /Users/chris/proj/LLM-based\n",
       "11   env.command_args  [-f, /Users/chris/Library/Jupyter/runtime/kernel-6a707c9e-9aec-4e96-abca-d6dadc46c3a3.json]\n",
       "12   env.num_ip_addrs                                                                                            0\n",
       "13    env.max_workers                                                                                            1\n",
       "14      env.debugging                                                                                        False\n",
       "15      env.msg_level                                                                                           20\n",
       "16     env.msg_format                                                                    %(asctime)s â”‡ %(message)s\n",
       "17    env.date_format                                                                             [%m.%d %H:%M:%S]\n",
       "18    env.output_home                                                                                         None\n",
       "19   env.logging_file                                                                                         None\n",
       "20  env.argument_file                                                                                         None\n",
       "21       time.started                                                                                         None\n",
       "22       time.settled                                                                                         None\n",
       "23       time.elapsed                                                                                         None"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CommonArguments</th>\n",
       "      <th>value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>tag</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>env.project</td>\n",
       "      <td>LLM-based</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>env.job_name</td>\n",
       "      <td>Llama 2 13b Chat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>env.job_version</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>env.hostname</td>\n",
       "      <td>ChrisBookPro.local</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>env.hostaddr</td>\n",
       "      <td>192.168.0.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>env.time_stamp</td>\n",
       "      <td>0430.203432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>env.python_path</td>\n",
       "      <td>/Users/chris/miniforge3/envs/LLM-based/bin/python3.11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>env.current_dir</td>\n",
       "      <td>/Users/chris/proj/LLM-based</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>env.current_file</td>\n",
       "      <td>/Users/chris/proj/LLM-based/Getting_to_know_Llama_2.ipynb</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>env.working_dir</td>\n",
       "      <td>/Users/chris/proj/LLM-based</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>env.command_args</td>\n",
       "      <td>[-f, /Users/chris/Library/Jupyter/runtime/kernel-6a707c9e-9aec-4e96-abca-d6dadc46c3a3.json]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>env.num_ip_addrs</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>env.max_workers</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>env.debugging</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>env.msg_level</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>env.msg_format</td>\n",
       "      <td>%(asctime)s â”‡ %(message)s</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>env.date_format</td>\n",
       "      <td>[%m.%d %H:%M:%S]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>env.output_home</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>env.logging_file</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>env.argument_file</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>time.started</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>time.settled</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>time.elapsed</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "id": "3a291ba614f530a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-30T11:34:33.435335Z",
     "start_time": "2024-04-30T11:34:33.402687Z"
    }
   },
   "source": [
    "# we will use replicate's hosted api\n",
    "import replicate\n",
    "\n",
    "\n",
    "# text completion with input prompt\n",
    "def Completion(prompt):\n",
    "    output = replicate.run(\n",
    "        model_ref,\n",
    "        input={\"prompt\": prompt,\n",
    "               \"max_new_tokens\": 1000}\n",
    "    )\n",
    "    return \"\".join(output)\n",
    "\n",
    "\n",
    "# chat completion with input prompt and system prompt\n",
    "def ChatCompletion(prompt, system_prompt=None):\n",
    "    output = replicate.run(\n",
    "        model_ref,\n",
    "        input={\"system_prompt\": system_prompt,\n",
    "               \"prompt\": prompt,\n",
    "               \"max_new_tokens\": 1000}\n",
    "    )\n",
    "    return \"\".join(output)"
   ],
   "outputs": [],
   "execution_count": 7
  },
  {
   "cell_type": "markdown",
   "id": "b8e638019cbe3f81",
   "metadata": {},
   "source": [
    "### **2.2 - Basic completion**"
   ]
  },
  {
   "cell_type": "code",
   "id": "20f09001024cc389",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-30T11:34:37.423341Z",
     "start_time": "2024-04-30T11:34:33.435945Z"
    }
   },
   "source": [
    "with JobTimer(\"2.2 - Basic completion\", rt=1, rb=1, rw=80, rc='=', verbose=1):\n",
    "    output = Completion(prompt=\"The typical color of a llama is: \")\n",
    "    logger.info(f\"LLM: {output}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[04.30 20:34:33] â”‡ ================================================================================\n",
      "[04.30 20:34:33] â”‡ [INIT] 2.2 - Basic completion\n",
      "[04.30 20:34:33] â”‡ ================================================================================\n",
      "[04.30 20:34:34] â”‡ HTTP Request: POST https://api.replicate.com/v1/predictions \"HTTP/1.1 201 Created\"\n",
      "[04.30 20:34:34] â”‡ HTTP Request: GET https://api.replicate.com/v1/models/meta/llama-2-13b-chat/versions/f4e2de70d66816a838a89eeeb621910adffb0dd0baba3976c96980970978018d \"HTTP/1.1 200 OK\"\n",
      "[04.30 20:34:35] â”‡ HTTP Request: GET https://api.replicate.com/v1/predictions/1yeth8ftznrgp0cf5v485f9png \"HTTP/1.1 200 OK\"\n",
      "[04.30 20:34:36] â”‡ HTTP Request: GET https://api.replicate.com/v1/predictions/1yeth8ftznrgp0cf5v485f9png \"HTTP/1.1 200 OK\"\n",
      "[04.30 20:34:37] â”‡ LLM:  Hello! I'm here to help answer your questions safely and helpfully. The typical color of a llama is a light beige or tan color, with some having a more grayish tint. However, it's important to note that llamas can come in a variety of colors, including white, black, and brown, as well as a range of shades in between. So, while the typical color of a llama may be light beige, there is no one \"correct\" color for all llamas. Is there anything else I can help with? ðŸ˜Š\n",
      "[04.30 20:34:37] â”‡ ================================================================================\n",
      "[04.30 20:34:37] â”‡ [EXIT] 2.2 - Basic completion ($=00:00:03.151)\n",
      "[04.30 20:34:37] â”‡ ================================================================================\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "cell_type": "markdown",
   "id": "c49e96ea313fa040",
   "metadata": {},
   "source": [
    "### **2.3 - System prompts**"
   ]
  },
  {
   "cell_type": "code",
   "id": "3393c643c521904",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-30T11:34:39.572640Z",
     "start_time": "2024-04-30T11:34:37.425639Z"
    }
   },
   "source": [
    "with JobTimer(\"2.3 - System prompts\", rt=1, rb=1, rw=80, rc='=', verbose=1):\n",
    "    output = ChatCompletion(\n",
    "        prompt=\"The typical color of a llama is: \",\n",
    "        system_prompt=\"respond with only one word\"\n",
    "    )\n",
    "    logger.info(f\"LLM: {output}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[04.30 20:34:37] â”‡ ================================================================================\n",
      "[04.30 20:34:37] â”‡ [INIT] 2.3 - System prompts\n",
      "[04.30 20:34:37] â”‡ ================================================================================\n",
      "[04.30 20:34:38] â”‡ HTTP Request: POST https://api.replicate.com/v1/predictions \"HTTP/1.1 201 Created\"\n",
      "[04.30 20:34:38] â”‡ HTTP Request: GET https://api.replicate.com/v1/models/meta/llama-2-13b-chat/versions/f4e2de70d66816a838a89eeeb621910adffb0dd0baba3976c96980970978018d \"HTTP/1.1 200 OK\"\n",
      "[04.30 20:34:39] â”‡ HTTP Request: GET https://api.replicate.com/v1/predictions/g1fyqr88m1rgm0cf5v4r19y550 \"HTTP/1.1 200 OK\"\n",
      "[04.30 20:34:39] â”‡ LLM:  Sure! Here's my response:\n",
      "\n",
      "Brown\n",
      "[04.30 20:34:39] â”‡ ================================================================================\n",
      "[04.30 20:34:39] â”‡ [EXIT] 2.3 - System prompts ($=00:00:01.301)\n",
      "[04.30 20:34:39] â”‡ ================================================================================\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "cell_type": "markdown",
   "id": "91dfc3b108520876",
   "metadata": {},
   "source": [
    "### **2.4 - Response formats**\n",
    "* Can support different formatted outputs e.g. text, JSON, etc."
   ]
  },
  {
   "cell_type": "code",
   "id": "2cdbd98ad3f5cda9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-30T11:34:42.239432Z",
     "start_time": "2024-04-30T11:34:39.575613Z"
    }
   },
   "source": [
    "with JobTimer(\"2.4 - Response formats\", rt=1, rb=1, rw=80, rc='=', verbose=1):\n",
    "    output = ChatCompletion(\n",
    "        prompt=\"The typical color of a llama is: \",\n",
    "        system_prompt=\"response in json format\"\n",
    "    )\n",
    "    logger.info(f\"LLM: {output}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[04.30 20:34:39] â”‡ ================================================================================\n",
      "[04.30 20:34:39] â”‡ [INIT] 2.4 - Response formats\n",
      "[04.30 20:34:39] â”‡ ================================================================================\n",
      "[04.30 20:34:40] â”‡ HTTP Request: POST https://api.replicate.com/v1/predictions \"HTTP/1.1 201 Created\"\n",
      "[04.30 20:34:40] â”‡ HTTP Request: GET https://api.replicate.com/v1/models/meta/llama-2-13b-chat/versions/f4e2de70d66816a838a89eeeb621910adffb0dd0baba3976c96980970978018d \"HTTP/1.1 200 OK\"\n",
      "[04.30 20:34:41] â”‡ HTTP Request: GET https://api.replicate.com/v1/predictions/c9h0eq0jjxrgp0cf5v4sps6754 \"HTTP/1.1 200 OK\"\n",
      "[04.30 20:34:41] â”‡ LLM:  {\n",
      "\"answer\": \"The typical color of a llama is brown.\"\n",
      "}\n",
      "[04.30 20:34:42] â”‡ ================================================================================\n",
      "[04.30 20:34:42] â”‡ [EXIT] 2.4 - Response formats ($=00:00:01.816)\n",
      "[04.30 20:34:42] â”‡ ================================================================================\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## **3 - Gen AI Application Architecture**\n",
    "\n",
    "Here is the high-level tech stack/architecture of Generative AI application."
   ],
   "id": "9a0a627dd9fdb44c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-30T11:34:42.249282Z",
     "start_time": "2024-04-30T11:34:42.243834Z"
    }
   },
   "cell_type": "code",
   "source": "genai_app_arch()",
   "id": "b7175c90b89a9cac",
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://mermaid.ink/img/CiAgICBmbG93Y2hhcnQgVEQKICAgICAgICBBW1VzZXJzXSAtLT4gQihBcHBsaWNhdGlvbnMgZS5nLiBtb2JpbGUsIHdlYikKICAgICAgICBCIC0tPiB8SG9zdGVkIEFQSXxDKFBsYXRmb3JtcyBlLmcuIEN1c3RvbSwgSHVnZ2luZ0ZhY2UsIFJlcGxpY2F0ZSkKICAgICAgICBCIC0tIG9wdGlvbmFsIC0tPiBFKEZyYW1ld29ya3MgZS5nLiBMYW5nQ2hhaW4pCiAgICAgICAgQy0tPnxVc2VyIElucHV0fERbTGxhbWEgMl0KICAgICAgICBELS0+fE1vZGVsIE91dHB1dHxDCiAgICAgICAgRSAtLT4gQwogICAgICAgIGNsYXNzRGVmIGRlZmF1bHQgZmlsbDojQ0NFNkZGLHN0cm9rZTojODRCQ0Y1LHRleHRDb2xvcjojMUMyQjMzLGZvbnRGYW1pbHk6dHJlYnVjaGV0IG1zOwogICAg\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## **4 - Chatbot Architecture**\n",
    "\n",
    "Here are the key components and the information flow in a chatbot.\n",
    "* User Prompts\n",
    "* Input Safety\n",
    "* Llama 2\n",
    "* Output Safety\n",
    "* Memory & Context"
   ],
   "id": "7ca1709c6ac20994"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-30T11:34:42.253931Z",
     "start_time": "2024-04-30T11:34:42.250477Z"
    }
   },
   "cell_type": "code",
   "source": "bot_arch()",
   "id": "daab8e849bb8fdb2",
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://mermaid.ink/img/CiAgICBncmFwaCBMUjsKICAgICAgICB1c2VyIC0tPiBwcm9tcHQKICAgICAgICBwcm9tcHQgLS0+IGlfc2FmZXR5CiAgICAgICAgaV9zYWZldHkgLS0+IGNvbnRleHQKICAgICAgICBjb250ZXh0IC0tPiBMbGFtYV8yCiAgICAgICAgTGxhbWFfMiAtLT4gb3V0cHV0CiAgICAgICAgb3V0cHV0IC0tPiBvX3NhZmV0eQogICAgICAgIGlfc2FmZXR5IC0tPiBtZW1vcnkKICAgICAgICBvX3NhZmV0eSAtLT4gbWVtb3J5CiAgICAgICAgbWVtb3J5IC0tPiBjb250ZXh0CiAgICAgICAgb19zYWZldHkgLS0+IHVzZXIKICAgICAgICBjbGFzc0RlZiBkZWZhdWx0IGZpbGw6I0NDRTZGRixzdHJva2U6Izg0QkNGNSx0ZXh0Q29sb3I6IzFDMkIzMyxmb250RmFtaWx5OnRyZWJ1Y2hldCBtczsKICAgIA==\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### **4.1 - Chat conversation**\n",
    "* LLMs are stateless\n",
    "* Single Turn\n",
    "* Multi Turn (Memory)"
   ],
   "id": "7cafbf05e3b4c9c0"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-30T11:34:44.484513Z",
     "start_time": "2024-04-30T11:34:42.255605Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# example of single turn chat\n",
    "with JobTimer(\"4.1 - Chat conversation\", rt=1, rb=1, rw=80, rc='=', verbose=1):\n",
    "    output = ChatCompletion(\n",
    "        prompt=\"What is the average lifespan of a Llama?\",\n",
    "        system_prompt=\"answer the last question in few words\"\n",
    "    )\n",
    "    logger.info(f\"LLM: {output}\")"
   ],
   "id": "e251253363dea6d0",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[04.30 20:34:42] â”‡ ================================================================================\n",
      "[04.30 20:34:42] â”‡ [INIT] 4.1 - Chat conversation\n",
      "[04.30 20:34:42] â”‡ ================================================================================\n",
      "[04.30 20:34:42] â”‡ HTTP Request: POST https://api.replicate.com/v1/predictions \"HTTP/1.1 201 Created\"\n",
      "[04.30 20:34:43] â”‡ HTTP Request: GET https://api.replicate.com/v1/models/meta/llama-2-13b-chat/versions/f4e2de70d66816a838a89eeeb621910adffb0dd0baba3976c96980970978018d \"HTTP/1.1 200 OK\"\n",
      "[04.30 20:34:44] â”‡ HTTP Request: GET https://api.replicate.com/v1/predictions/9f5mwmgvfhrgp0cf5v4r4y1tr8 \"HTTP/1.1 200 OK\"\n",
      "[04.30 20:34:44] â”‡ LLM:  Sure! The average lifespan of a llama is 15-20 years.\n",
      "[04.30 20:34:44] â”‡ ================================================================================\n",
      "[04.30 20:34:44] â”‡ [EXIT] 4.1 - Chat conversation ($=00:00:01.392)\n",
      "[04.30 20:34:44] â”‡ ================================================================================\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-30T11:34:47.152440Z",
     "start_time": "2024-04-30T11:34:44.486356Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# example of single turn chat\n",
    "with JobTimer(\"4.1 - Chat conversation: Single Turn (1/2)\", rt=1, rb=1, rw=80, rc='=', verbose=1):\n",
    "    output = ChatCompletion(\n",
    "        prompt=\"What is the average lifespan of a Llama?\",\n",
    "        system_prompt=\"answer the last question in few words\"\n",
    "    )\n",
    "html(f\"<div class='LLM'>{output}</div>\")"
   ],
   "id": "bdb99fc9883cb8e6",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[04.30 20:34:44] â”‡ ================================================================================\n",
      "[04.30 20:34:44] â”‡ [INIT] 4.1 - Chat conversation: Single Turn (1/2)\n",
      "[04.30 20:34:44] â”‡ ================================================================================\n",
      "[04.30 20:34:45] â”‡ HTTP Request: POST https://api.replicate.com/v1/predictions \"HTTP/1.1 201 Created\"\n",
      "[04.30 20:34:45] â”‡ HTTP Request: GET https://api.replicate.com/v1/models/meta/llama-2-13b-chat/versions/f4e2de70d66816a838a89eeeb621910adffb0dd0baba3976c96980970978018d \"HTTP/1.1 200 OK\"\n",
      "[04.30 20:34:46] â”‡ HTTP Request: GET https://api.replicate.com/v1/predictions/qwp1ndh485rgp0cf5v4tt16jzr \"HTTP/1.1 200 OK\"\n",
      "[04.30 20:34:46] â”‡ ================================================================================\n",
      "[04.30 20:34:46] â”‡ [EXIT] 4.1 - Chat conversation: Single Turn (1/2) ($=00:00:01.819)\n",
      "[04.30 20:34:46] â”‡ ================================================================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "HTML(value=\"<div class='LLM'> Sure! The average lifespan of a llama is 15 to 20 years.</div>\")"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "3f507ec44c1e40b9bc82bffbc820c170"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-30T11:34:49.733493Z",
     "start_time": "2024-04-30T11:34:47.153681Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# example without previous context. LLM's are stateless and cannot understand \"they\" without previous context\n",
    "with JobTimer(\"4.1 - Chat conversation: Single Turn (2/2)\", rt=1, rb=1, rw=80, rc='=', verbose=1):\n",
    "    output = ChatCompletion(\n",
    "        prompt=\"What animal family are they?\",\n",
    "        system_prompt=\"answer the last question in few words\"\n",
    "    )\n",
    "html(f\"<div class='LLM'>{output}</div>\")"
   ],
   "id": "8530c9c8b79f9eaf",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[04.30 20:34:47] â”‡ ================================================================================\n",
      "[04.30 20:34:47] â”‡ [INIT] 4.1 - Chat conversation: Single Turn (2/2)\n",
      "[04.30 20:34:47] â”‡ ================================================================================\n",
      "[04.30 20:34:47] â”‡ HTTP Request: POST https://api.replicate.com/v1/predictions \"HTTP/1.1 201 Created\"\n",
      "[04.30 20:34:48] â”‡ HTTP Request: GET https://api.replicate.com/v1/models/meta/llama-2-13b-chat/versions/f4e2de70d66816a838a89eeeb621910adffb0dd0baba3976c96980970978018d \"HTTP/1.1 200 OK\"\n",
      "[04.30 20:34:49] â”‡ HTTP Request: GET https://api.replicate.com/v1/predictions/bnm1b69en1rgp0cf5v4rd3m83r \"HTTP/1.1 200 OK\"\n",
      "[04.30 20:34:49] â”‡ ================================================================================\n",
      "[04.30 20:34:49] â”‡ [EXIT] 4.1 - Chat conversation: Single Turn (2/2) ($=00:00:01.731)\n",
      "[04.30 20:34:49] â”‡ ================================================================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "HTML(value=\"<div class='LLM'> Sure! Here's the answer in a few words:\\n\\nThey are a type of canine.</div>\")"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "1b17f23a5c784ed888eb100f41628333"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Chat app requires us to send in previous context to LLM to get in valid responses. Below is an example of Multi-turn chat.",
   "id": "5ed6c2e295ea021b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-30T11:34:51.836817Z",
     "start_time": "2024-04-30T11:34:49.735480Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# example of single turn chat\n",
    "with JobTimer(\"4.1 - Chat conversation: Multi Turn (Memory)\", rt=1, rb=1, rw=80, rc='=', verbose=1):\n",
    "    output = ChatCompletion(\n",
    "        system_prompt=\"answer the last question in few words\",\n",
    "        prompt=\"\"\"\n",
    "User: What is the average lifespan of a Llama?\n",
    "Assistant: Sure! The average lifespan of a llama is around 20-30 years.\n",
    "User: What animal family are they?\n",
    "\"\"\",\n",
    "    )\n",
    "html(f\"<div class='LLM'>{output}</div>\")"
   ],
   "id": "8454cb422834ea24",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[04.30 20:34:49] â”‡ ================================================================================\n",
      "[04.30 20:34:49] â”‡ [INIT] 4.1 - Chat conversation: Multi Turn (Memory)\n",
      "[04.30 20:34:49] â”‡ ================================================================================\n",
      "[04.30 20:34:50] â”‡ HTTP Request: POST https://api.replicate.com/v1/predictions \"HTTP/1.1 201 Created\"\n",
      "[04.30 20:34:50] â”‡ HTTP Request: GET https://api.replicate.com/v1/models/meta/llama-2-13b-chat/versions/f4e2de70d66816a838a89eeeb621910adffb0dd0baba3976c96980970978018d \"HTTP/1.1 200 OK\"\n",
      "[04.30 20:34:51] â”‡ HTTP Request: GET https://api.replicate.com/v1/predictions/eecfjzsrr1rgg0cf5v4r93wq8r \"HTTP/1.1 200 OK\"\n",
      "[04.30 20:34:51] â”‡ ================================================================================\n",
      "[04.30 20:34:51] â”‡ [EXIT] 4.1 - Chat conversation: Multi Turn (Memory) ($=00:00:01.256)\n",
      "[04.30 20:34:51] â”‡ ================================================================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "HTML(value=\"<div class='LLM'> Sure! Llamas are members of the camelid family.</div>\")"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "7f4efa6230754afaa50115b644049f9e"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 16
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
