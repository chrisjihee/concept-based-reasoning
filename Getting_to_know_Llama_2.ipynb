{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d680961880222a81",
   "metadata": {},
   "source": [
    "# **Getting to know Llama 2: Everything you need to start building**\n",
    "Our goal in this session is to provide a guided tour of Llama 2, including understanding different Llama 2 models, how and where to access them, Generative AI and Chatbot architectures, prompt engineering, RAG (Retrieval Augmented Generation), Fine-tuning and more. All this is implemented with a starter code for you to take it and use it in your Llama 2 projects."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c545e17b77f35ab",
   "metadata": {},
   "source": [
    "## **0 - Prerequisites**\n",
    "* Basic understanding of Large Language Models\n",
    "* Basic understanding of Python"
   ]
  },
  {
   "cell_type": "code",
   "id": "d87dc3a5a98e8f7f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-01T05:17:34.457929Z",
     "start_time": "2024-05-01T05:17:33.642011Z"
    }
   },
   "source": [
    "import base64\n",
    "from getpass import getpass\n",
    "\n",
    "import replicate\n",
    "from IPython.display import display, Image, Markdown, HTML\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain.document_loaders import OnlinePDFLoader\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "from chrisbase.data import *\n",
    "from chrisbase.io import *\n",
    "from chrisbase.util import *\n",
    "\n",
    "\n",
    "def mm(graph):\n",
    "    graphbytes = graph.encode(\"ascii\")\n",
    "    base64_bytes = base64.b64encode(graphbytes)\n",
    "    base64_string = base64_bytes.decode(\"ascii\")\n",
    "    display(Image(url=\"https://mermaid.ink/img/\" + base64_string))\n",
    "\n",
    "\n",
    "def md(t):\n",
    "    display(Markdown(t))\n",
    "\n",
    "\n",
    "def html(t,\n",
    "         s=\"<div style='font-family:Arial; font-size:12pt;\"\n",
    "           \" padding:10px; margin-left:10px; margin-top:10px;\"\n",
    "           \" background-color:LightSkyBlue; border:3px solid MidnightBlue;'>\",\n",
    "         e=\"</div>\"):\n",
    "    display(HTML(s + t + e))\n",
    "\n",
    "\n",
    "def genai_app_arch():\n",
    "    mm(\"\"\"\n",
    "    flowchart TD\n",
    "        A[Users] --> B(Applications e.g. mobile, web)\n",
    "        B --> |Hosted API|C(Platforms e.g. Custom, HuggingFace, Replicate)\n",
    "        B -- optional --> E(Frameworks e.g. LangChain)\n",
    "        C-->|User Input|D[Llama 2]\n",
    "        D-->|Model Output|C\n",
    "        E --> C\n",
    "        classDef default fill:#CCE6FF,stroke:#84BCF5,textColor:#1C2B33,fontFamily:trebuchet ms;\n",
    "    \"\"\")\n",
    "\n",
    "\n",
    "def rag_arch():\n",
    "    mm(\"\"\"\n",
    "    flowchart TD\n",
    "        A[User Prompts] --> B(Frameworks e.g. LangChain)\n",
    "        B <--> |Database, Docs, XLS|C[fa:fa-database External Data]\n",
    "        B -->|API|D[Llama 2]\n",
    "        classDef default fill:#CCE6FF,stroke:#84BCF5,textColor:#1C2B33,fontFamily:trebuchet ms;\n",
    "    \"\"\")\n",
    "\n",
    "\n",
    "def llama2_family():\n",
    "    mm(\"\"\"\n",
    "    graph LR;\n",
    "        llama-2 --> llama-2-7b\n",
    "        llama-2 --> llama-2-13b\n",
    "        llama-2 --> llama-2-70b\n",
    "        llama-2-7b --> llama-2-7b-chat\n",
    "        llama-2-13b --> llama-2-13b-chat\n",
    "        llama-2-70b --> llama-2-70b-chat\n",
    "        classDef default fill:#CCE6FF,stroke:#84BCF5,textColor:#1C2B33,fontFamily:trebuchet ms;\n",
    "    \"\"\")\n",
    "\n",
    "\n",
    "def apps_and_llms():\n",
    "    mm(\"\"\"\n",
    "    graph LR;\n",
    "        users --> apps\n",
    "        apps --> frameworks\n",
    "        frameworks --> platforms\n",
    "        platforms --> Llama 2\n",
    "        classDef default fill:#CCE6FF,stroke:#84BCF5,textColor:#1C2B33,fontFamily:trebuchet ms;\n",
    "    \"\"\")\n",
    "\n",
    "\n",
    "def bot_arch():\n",
    "    mm(\"\"\"\n",
    "    graph LR;\n",
    "        user --> prompt\n",
    "        prompt --> i_safety\n",
    "        i_safety --> context\n",
    "        context --> Llama_2\n",
    "        Llama_2 --> output\n",
    "        output --> o_safety\n",
    "        i_safety --> memory\n",
    "        o_safety --> memory\n",
    "        memory --> context\n",
    "        o_safety --> user\n",
    "        classDef default fill:#CCE6FF,stroke:#84BCF5,textColor:#1C2B33,fontFamily:trebuchet ms;\n",
    "    \"\"\")\n",
    "\n",
    "\n",
    "def fine_tuned_arch():\n",
    "    mm(\"\"\"\n",
    "    graph LR;\n",
    "        Custom_Dataset --> Pre-trained_Llama\n",
    "        Pre-trained_Llama --> Fine-tuned_Llama\n",
    "        Fine-tuned_Llama --> RLHF\n",
    "        RLHF --> |Loss:Cross-Entropy|Fine-tuned_Llama\n",
    "        classDef default fill:#CCE6FF,stroke:#84BCF5,textColor:#1C2B33,fontFamily:trebuchet ms;\n",
    "    \"\"\")\n",
    "\n",
    "\n",
    "def load_data_faiss_arch():\n",
    "    mm(\"\"\"\n",
    "    graph LR;\n",
    "        documents --> textsplitter\n",
    "        textsplitter --> embeddings\n",
    "        embeddings --> vectorstore\n",
    "        classDef default fill:#CCE6FF,stroke:#84BCF5,textColor:#1C2B33,fontFamily:trebuchet ms;\n",
    "    \"\"\")\n",
    "\n",
    "\n",
    "def mem_context():\n",
    "    mm(\"\"\"\n",
    "    graph LR\n",
    "        context(text)\n",
    "        user_prompt --> context\n",
    "        instruction --> context\n",
    "        examples --> context\n",
    "        memory --> context\n",
    "        context --> tokenizer\n",
    "        tokenizer --> embeddings\n",
    "        embeddings --> LLM\n",
    "        classDef default fill:#CCE6FF,stroke:#84BCF5,textColor:#1C2B33,fontFamily:trebuchet ms;\n",
    "    \"\"\")\n",
    "\n",
    "\n",
    "# text completion with input prompt\n",
    "def Completion(prompt):\n",
    "    out = replicate.run(\n",
    "        model_ref,\n",
    "        input={\"prompt\": prompt,\n",
    "               \"max_new_tokens\": 1000}\n",
    "    )\n",
    "    return \"\".join(out)\n",
    "\n",
    "\n",
    "# chat completion with input prompt and system prompt\n",
    "def ChatCompletion(prompt, system_prompt=None):\n",
    "    out = replicate.run(\n",
    "        model_ref,\n",
    "        input={\"system_prompt\": system_prompt,\n",
    "               \"prompt\": prompt,\n",
    "               \"max_new_tokens\": 1000}\n",
    "    )\n",
    "    return \"\".join(out)"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "id": "acd794f15ce1f743",
   "metadata": {},
   "source": [
    "## **1 - Understanding Llama 2**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "548692b6a6dd66c",
   "metadata": {},
   "source": [
    "### **1.1 - What is Llama 2?**\n",
    "* State of the art (SOTA), Open Source LLM\n",
    "* 7B, 13B, 70B\n",
    "* Pretrained + Chat\n",
    "* Choosing model: Size, Quality, Cost, Speed\n",
    "* [Research paper](https://ai.meta.com/research/publications/llama-2-open-foundation-and-fine-tuned-chat-models/)\n",
    "* [Responsible use guide](https://ai.meta.com/llama/responsible-use-guide/)"
   ]
  },
  {
   "cell_type": "code",
   "id": "f7d7c15448e3f170",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-01T05:17:34.461877Z",
     "start_time": "2024-05-01T05:17:34.458986Z"
    }
   },
   "source": [
    "llama2_family()"
   ],
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://mermaid.ink/img/CiAgICBncmFwaCBMUjsKICAgICAgICBsbGFtYS0yIC0tPiBsbGFtYS0yLTdiCiAgICAgICAgbGxhbWEtMiAtLT4gbGxhbWEtMi0xM2IKICAgICAgICBsbGFtYS0yIC0tPiBsbGFtYS0yLTcwYgogICAgICAgIGxsYW1hLTItN2IgLS0+IGxsYW1hLTItN2ItY2hhdAogICAgICAgIGxsYW1hLTItMTNiIC0tPiBsbGFtYS0yLTEzYi1jaGF0CiAgICAgICAgbGxhbWEtMi03MGIgLS0+IGxsYW1hLTItNzBiLWNoYXQKICAgICAgICBjbGFzc0RlZiBkZWZhdWx0IGZpbGw6I0NDRTZGRixzdHJva2U6Izg0QkNGNSx0ZXh0Q29sb3I6IzFDMkIzMyxmb250RmFtaWx5OnRyZWJ1Y2hldCBtczsKICAgIA==\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "id": "190b25e8cdc4c64d",
   "metadata": {},
   "source": [
    "### **1.2 - Accessing Llama 2**\n",
    "* Download + Self Host (on-premise)\n",
    "* Hosted API Platform (e.g. [Replicate](https://replicate.com/meta))\n",
    "* Hosted Container Platform (e.g. [Azure](https://techcommunity.microsoft.com/t5/ai-machine-learning-blog/introducing-llama-2-on-azure/ba-p/3881233), [AWS](https://aws.amazon.com/blogs/machine-learning/llama-2-foundation-models-from-meta-are-now-available-in-amazon-sagemaker-jumpstart/), [GCP](https://console.cloud.google.com/vertex-ai/publishers/google/model-garden/139))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7f0f1cf7c500be4",
   "metadata": {},
   "source": [
    "### **1.3 - Use Cases of Llama 2**\n",
    "* Content Generation\n",
    "* Chatbots\n",
    "* Summarization\n",
    "* Programming (e.g. Code Llama)\n",
    "* and many more..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "965614383680fb8",
   "metadata": {},
   "source": [
    "## **2 - Using Llama 2**\n",
    "In this notebook, we are going to access [Llama 13b chat model](https://replicate.com/meta/llama-2-13b-chat) using hosted API from Replicate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67f1a112c1342b73",
   "metadata": {},
   "source": [
    "### **2.1 - Install dependencies**"
   ]
  },
  {
   "cell_type": "code",
   "id": "8a77a8bb71e58ed5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-01T05:17:35.030448Z",
     "start_time": "2024-05-01T05:17:34.462425Z"
    }
   },
   "source": [
    "# Install dependencies and initialize\n",
    "!pip list | grep -E \"torch|transformers|chris|langchain|replicate|faiss|jupyter\""
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chrisbase                 0.5.2\r\n",
      "faiss-cpu                 1.8.0\r\n",
      "jupyter                   1.0.0\r\n",
      "jupyter_client            8.6.1\r\n",
      "jupyter-console           6.6.3\r\n",
      "jupyter_core              5.7.2\r\n",
      "jupyter-events            0.10.0\r\n",
      "jupyter-lsp               2.2.5\r\n",
      "jupyter_server            2.14.0\r\n",
      "jupyter_server_terminals  0.5.3\r\n",
      "jupyterlab                4.1.8\r\n",
      "jupyterlab_pygments       0.3.0\r\n",
      "jupyterlab_server         2.27.1\r\n",
      "jupyterlab_widgets        3.0.10\r\n",
      "langchain                 0.1.16\r\n",
      "langchain-community       0.0.34\r\n",
      "langchain-core            0.1.46\r\n",
      "langchain-text-splitters  0.0.1\r\n",
      "replicate                 0.25.2\r\n",
      "sentence-transformers     2.7.0\r\n",
      "torch                     2.3.0\r\n",
      "transformers              4.40.1\r\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "id": "8068c18c446acaec",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-01T05:17:35.034298Z",
     "start_time": "2024-05-01T05:17:35.031926Z"
    }
   },
   "source": [
    "# model url on Replicate platform that we will use for inferencing\n",
    "# We will use llama 2 13b chat model hosted on replicate server ()agent_name = \"llama-2-13b-chat\"\n",
    "model_ref = f\"meta/llama-2-13b-chat:f4e2de70d66816a838a89eeeb621910adffb0dd0baba3976c96980970978018d\""
   ],
   "outputs": [],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "id": "45ec8c200c5b6f57",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-01T05:17:35.055389Z",
     "start_time": "2024-05-01T05:17:35.036293Z"
    }
   },
   "source": [
    "# We will use Replicate hosted cloud environment\n",
    "# Obtain Replicate API key ‚Üí https://replicate.com/account/api-tokens)\n",
    "\n",
    "# enter your replicate api token\n",
    "REPLICATE_API_TOKEN = read_or(first_path_or(\".replicate*\")) or getpass()\n",
    "os.environ[\"REPLICATE_API_TOKEN\"] = REPLICATE_API_TOKEN\n",
    "\n",
    "logging.getLogger(\"IPKernelApp\").setLevel(logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "args = CommonArguments(\n",
    "    env=ProjectEnv(\n",
    "        project=\"LLM-based\",\n",
    "        job_name=\"LLaMA-2-13B-Chat\",\n",
    "        msg_level=logging.INFO,\n",
    "        msg_format=LoggingFormat.BRIEF_00,\n",
    "    )\n",
    ")\n",
    "args.dataframe()"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "      CommonArguments                                                                                        value\n",
       "0                 tag                                                                                         None\n",
       "1         env.project                                                                                    LLM-based\n",
       "2        env.job_name                                                                             LLaMA-2-13B-Chat\n",
       "3     env.job_version                                                                                         None\n",
       "4        env.hostname                                                                           ChrisBookPro.local\n",
       "5        env.hostaddr                                                                                  172.20.10.5\n",
       "6      env.time_stamp                                                                                  0501.141734\n",
       "7     env.python_path                                        /Users/chris/miniforge3/envs/LLM-based/bin/python3.11\n",
       "8     env.current_dir                                                                  /Users/chris/proj/LLM-based\n",
       "9    env.current_file                                    /Users/chris/proj/LLM-based/Getting_to_know_Llama_2.ipynb\n",
       "10    env.working_dir                                                                  /Users/chris/proj/LLM-based\n",
       "11   env.command_args  [-f, /Users/chris/Library/Jupyter/runtime/kernel-0a542593-6161-464b-8fff-646fae8f56f1.json]\n",
       "12   env.num_ip_addrs                                                                                            0\n",
       "13    env.max_workers                                                                                            1\n",
       "14      env.debugging                                                                                        False\n",
       "15      env.msg_level                                                                                           20\n",
       "16     env.msg_format                                                                    %(asctime)s ‚îá %(message)s\n",
       "17    env.date_format                                                                             [%m.%d %H:%M:%S]\n",
       "18    env.output_home                                                                                         None\n",
       "19   env.logging_file                                                                                         None\n",
       "20  env.argument_file                                                                                         None\n",
       "21       time.started                                                                                         None\n",
       "22       time.settled                                                                                         None\n",
       "23       time.elapsed                                                                                         None"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CommonArguments</th>\n",
       "      <th>value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>tag</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>env.project</td>\n",
       "      <td>LLM-based</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>env.job_name</td>\n",
       "      <td>LLaMA-2-13B-Chat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>env.job_version</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>env.hostname</td>\n",
       "      <td>ChrisBookPro.local</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>env.hostaddr</td>\n",
       "      <td>172.20.10.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>env.time_stamp</td>\n",
       "      <td>0501.141734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>env.python_path</td>\n",
       "      <td>/Users/chris/miniforge3/envs/LLM-based/bin/python3.11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>env.current_dir</td>\n",
       "      <td>/Users/chris/proj/LLM-based</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>env.current_file</td>\n",
       "      <td>/Users/chris/proj/LLM-based/Getting_to_know_Llama_2.ipynb</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>env.working_dir</td>\n",
       "      <td>/Users/chris/proj/LLM-based</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>env.command_args</td>\n",
       "      <td>[-f, /Users/chris/Library/Jupyter/runtime/kernel-0a542593-6161-464b-8fff-646fae8f56f1.json]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>env.num_ip_addrs</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>env.max_workers</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>env.debugging</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>env.msg_level</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>env.msg_format</td>\n",
       "      <td>%(asctime)s ‚îá %(message)s</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>env.date_format</td>\n",
       "      <td>[%m.%d %H:%M:%S]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>env.output_home</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>env.logging_file</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>env.argument_file</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>time.started</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>time.settled</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>time.elapsed</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "id": "b8e638019cbe3f81",
   "metadata": {},
   "source": [
    "### **2.2 - Basic completion**"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-01T05:17:39.685544Z",
     "start_time": "2024-05-01T05:17:35.056173Z"
    }
   },
   "cell_type": "code",
   "source": [
    "with JobTimer(\"2.2 - Basic completion\", rt=1, rb=1, rw=80, rc='=', verbose=1):\n",
    "    output = Completion(prompt=\"The typical color of a llama is: \")\n",
    "html(output)"
   ],
   "id": "5ddec5b7cf4d4873",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[05.01 14:17:35] ‚îá ================================================================================\n",
      "[05.01 14:17:35] ‚îá [INIT] 2.2 - Basic completion\n",
      "[05.01 14:17:35] ‚îá ================================================================================\n",
      "[05.01 14:17:37] ‚îá HTTP Request: POST https://api.replicate.com/v1/predictions \"HTTP/1.1 201 Created\"\n",
      "[05.01 14:17:37] ‚îá HTTP Request: GET https://api.replicate.com/v1/models/meta/llama-2-13b-chat/versions/f4e2de70d66816a838a89eeeb621910adffb0dd0baba3976c96980970978018d \"HTTP/1.1 200 OK\"\n",
      "[05.01 14:17:38] ‚îá HTTP Request: GET https://api.replicate.com/v1/predictions/dg2d5nhrgsrgm0cf6abb0kqfp4 \"HTTP/1.1 200 OK\"\n",
      "[05.01 14:17:39] ‚îá HTTP Request: GET https://api.replicate.com/v1/predictions/dg2d5nhrgsrgm0cf6abb0kqfp4 \"HTTP/1.1 200 OK\"\n",
      "[05.01 14:17:39] ‚îá ================================================================================\n",
      "[05.01 14:17:39] ‚îá [EXIT] 2.2 - Basic completion ($=00:00:03.786)\n",
      "[05.01 14:17:39] ‚îá ================================================================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "<div style='font-family:Arial; font-size:12pt; padding:10px; margin-left:10px; margin-top:10px; background-color:LightSkyBlue; border:3px solid MidnightBlue;'> Hello! I'm here to help. The typical color of a llama is a light brown or beige color, with some individuals having a more reddish or grayish tint to their fur. However, it's important to note that llama fur can vary in color and pattern, and some llamas may have white, dark brown, or black markings on their faces or bodies. Additionally, some breeds of llamas, such as the Suri llama, have a more distinctive, curly coat that can come in a variety of colors. Is there anything else you'd like to know? üòä</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 6
  },
  {
   "cell_type": "markdown",
   "id": "c49e96ea313fa040",
   "metadata": {},
   "source": [
    "### **2.3 - System prompts**"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-01T05:17:43.254896Z",
     "start_time": "2024-05-01T05:17:39.686398Z"
    }
   },
   "cell_type": "code",
   "source": [
    "with JobTimer(\"2.3 - System prompts\", rt=1, rb=1, rw=80, rc='=', verbose=1):\n",
    "    output = ChatCompletion(\n",
    "        prompt=\"The typical color of a llama is: \",\n",
    "        system_prompt=\"respond with only one word\",\n",
    "    )\n",
    "html(output)"
   ],
   "id": "620a65350d87ad2f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[05.01 14:17:39] ‚îá ================================================================================\n",
      "[05.01 14:17:39] ‚îá [INIT] 2.3 - System prompts\n",
      "[05.01 14:17:39] ‚îá ================================================================================\n",
      "[05.01 14:17:40] ‚îá HTTP Request: POST https://api.replicate.com/v1/predictions \"HTTP/1.1 201 Created\"\n",
      "[05.01 14:17:41] ‚îá HTTP Request: GET https://api.replicate.com/v1/models/meta/llama-2-13b-chat/versions/f4e2de70d66816a838a89eeeb621910adffb0dd0baba3976c96980970978018d \"HTTP/1.1 200 OK\"\n",
      "[05.01 14:17:42] ‚îá HTTP Request: GET https://api.replicate.com/v1/predictions/qeyy1ta6rxrgm0cf6ab9zep00w \"HTTP/1.1 200 OK\"\n",
      "[05.01 14:17:43] ‚îá ================================================================================\n",
      "[05.01 14:17:43] ‚îá [EXIT] 2.3 - System prompts ($=00:00:02.731)\n",
      "[05.01 14:17:43] ‚îá ================================================================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "<div style='font-family:Arial; font-size:12pt; padding:10px; margin-left:10px; margin-top:10px; background-color:LightSkyBlue; border:3px solid MidnightBlue;'> Sure! Here's my response:\n",
       "\n",
       "Brown</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 7
  },
  {
   "cell_type": "markdown",
   "id": "91dfc3b108520876",
   "metadata": {},
   "source": [
    "### **2.4 - Response formats**\n",
    "* Can support different formatted outputs e.g. text, JSON, etc."
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-01T05:17:46.994537Z",
     "start_time": "2024-05-01T05:17:43.255802Z"
    }
   },
   "cell_type": "code",
   "source": [
    "with JobTimer(\"2.4 - Response formats\", rt=1, rb=1, rw=80, rc='=', verbose=1):\n",
    "    output = ChatCompletion(\n",
    "        prompt=\"The typical color of a llama is: \",\n",
    "        system_prompt=\"response in json format\",\n",
    "    )\n",
    "html(output)"
   ],
   "id": "971798e7feb638d0",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[05.01 14:17:43] ‚îá ================================================================================\n",
      "[05.01 14:17:43] ‚îá [INIT] 2.4 - Response formats\n",
      "[05.01 14:17:43] ‚îá ================================================================================\n",
      "[05.01 14:17:44] ‚îá HTTP Request: POST https://api.replicate.com/v1/predictions \"HTTP/1.1 201 Created\"\n",
      "[05.01 14:17:45] ‚îá HTTP Request: GET https://api.replicate.com/v1/models/meta/llama-2-13b-chat/versions/f4e2de70d66816a838a89eeeb621910adffb0dd0baba3976c96980970978018d \"HTTP/1.1 200 OK\"\n",
      "[05.01 14:17:46] ‚îá HTTP Request: GET https://api.replicate.com/v1/predictions/v7p03h2ms1rgg0cf6ab952rjy4 \"HTTP/1.1 200 OK\"\n",
      "[05.01 14:17:46] ‚îá ================================================================================\n",
      "[05.01 14:17:46] ‚îá [EXIT] 2.4 - Response formats ($=00:00:02.903)\n",
      "[05.01 14:17:46] ‚îá ================================================================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "<div style='font-family:Arial; font-size:12pt; padding:10px; margin-left:10px; margin-top:10px; background-color:LightSkyBlue; border:3px solid MidnightBlue;'> {\n",
       "\"response\": {\n",
       "\"typical_color\": \"white\"\n",
       "}\n",
       "}</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## **3 - Gen AI Application Architecture**\n",
    "\n",
    "Here is the high-level tech stack/architecture of Generative AI application."
   ],
   "id": "9a0a627dd9fdb44c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-01T05:17:46.997920Z",
     "start_time": "2024-05-01T05:17:46.995332Z"
    }
   },
   "cell_type": "code",
   "source": "genai_app_arch()",
   "id": "b7175c90b89a9cac",
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://mermaid.ink/img/CiAgICBmbG93Y2hhcnQgVEQKICAgICAgICBBW1VzZXJzXSAtLT4gQihBcHBsaWNhdGlvbnMgZS5nLiBtb2JpbGUsIHdlYikKICAgICAgICBCIC0tPiB8SG9zdGVkIEFQSXxDKFBsYXRmb3JtcyBlLmcuIEN1c3RvbSwgSHVnZ2luZ0ZhY2UsIFJlcGxpY2F0ZSkKICAgICAgICBCIC0tIG9wdGlvbmFsIC0tPiBFKEZyYW1ld29ya3MgZS5nLiBMYW5nQ2hhaW4pCiAgICAgICAgQy0tPnxVc2VyIElucHV0fERbTGxhbWEgMl0KICAgICAgICBELS0+fE1vZGVsIE91dHB1dHxDCiAgICAgICAgRSAtLT4gQwogICAgICAgIGNsYXNzRGVmIGRlZmF1bHQgZmlsbDojQ0NFNkZGLHN0cm9rZTojODRCQ0Y1LHRleHRDb2xvcjojMUMyQjMzLGZvbnRGYW1pbHk6dHJlYnVjaGV0IG1zOwogICAg\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## **4 - Chatbot Architecture**\n",
    "\n",
    "Here are the key components and the information flow in a chatbot.\n",
    "* User Prompts\n",
    "* Input Safety\n",
    "* Llama 2\n",
    "* Output Safety\n",
    "* Memory & Context"
   ],
   "id": "7ca1709c6ac20994"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-01T05:17:47.000951Z",
     "start_time": "2024-05-01T05:17:46.998712Z"
    }
   },
   "cell_type": "code",
   "source": "bot_arch()",
   "id": "daab8e849bb8fdb2",
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://mermaid.ink/img/CiAgICBncmFwaCBMUjsKICAgICAgICB1c2VyIC0tPiBwcm9tcHQKICAgICAgICBwcm9tcHQgLS0+IGlfc2FmZXR5CiAgICAgICAgaV9zYWZldHkgLS0+IGNvbnRleHQKICAgICAgICBjb250ZXh0IC0tPiBMbGFtYV8yCiAgICAgICAgTGxhbWFfMiAtLT4gb3V0cHV0CiAgICAgICAgb3V0cHV0IC0tPiBvX3NhZmV0eQogICAgICAgIGlfc2FmZXR5IC0tPiBtZW1vcnkKICAgICAgICBvX3NhZmV0eSAtLT4gbWVtb3J5CiAgICAgICAgbWVtb3J5IC0tPiBjb250ZXh0CiAgICAgICAgb19zYWZldHkgLS0+IHVzZXIKICAgICAgICBjbGFzc0RlZiBkZWZhdWx0IGZpbGw6I0NDRTZGRixzdHJva2U6Izg0QkNGNSx0ZXh0Q29sb3I6IzFDMkIzMyxmb250RmFtaWx5OnRyZWJ1Y2hldCBtczsKICAgIA==\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### **4.1 - Chat conversation**\n",
    "* LLMs are stateless\n",
    "* Single Turn\n",
    "* Multi Turn (Memory)"
   ],
   "id": "7cafbf05e3b4c9c0"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-01T05:17:50.997926Z",
     "start_time": "2024-05-01T05:17:47.001548Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# example of single turn chat\n",
    "prompt_chat = \"What is the average lifespan of a Llama?\"\n",
    "with JobTimer(\"4.1 - Chat conversation: Single Turn [1/2]\", rt=1, rb=1, rw=80, rc='=', verbose=1):\n",
    "    output = ChatCompletion(prompt=prompt_chat, system_prompt=\"answer the last question in few words\")\n",
    "html(output)"
   ],
   "id": "ce5be38fb3a6de58",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[05.01 14:17:47] ‚îá ================================================================================\n",
      "[05.01 14:17:47] ‚îá [INIT] 4.1 - Chat conversation: Single Turn [1/2]\n",
      "[05.01 14:17:47] ‚îá ================================================================================\n",
      "[05.01 14:17:48] ‚îá HTTP Request: POST https://api.replicate.com/v1/predictions \"HTTP/1.1 201 Created\"\n",
      "[05.01 14:17:49] ‚îá HTTP Request: GET https://api.replicate.com/v1/models/meta/llama-2-13b-chat/versions/f4e2de70d66816a838a89eeeb621910adffb0dd0baba3976c96980970978018d \"HTTP/1.1 200 OK\"\n",
      "[05.01 14:17:50] ‚îá HTTP Request: GET https://api.replicate.com/v1/predictions/7hbk1gk5hdrgm0cf6ab8p0qkg0 \"HTTP/1.1 200 OK\"\n",
      "[05.01 14:17:50] ‚îá ================================================================================\n",
      "[05.01 14:17:50] ‚îá [EXIT] 4.1 - Chat conversation: Single Turn [1/2] ($=00:00:03.159)\n",
      "[05.01 14:17:50] ‚îá ================================================================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "<div style='font-family:Arial; font-size:12pt; padding:10px; margin-left:10px; margin-top:10px; background-color:LightSkyBlue; border:3px solid MidnightBlue;'> Sure! The average lifespan of a llama is around 15-20 years.</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-01T05:17:54.999678Z",
     "start_time": "2024-05-01T05:17:50.999020Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# example without previous context. LLM's are stateless and cannot understand \"they\" without previous context\n",
    "prompt_chat = \"What animal family are they?\"\n",
    "with JobTimer(\"4.1 - Chat conversation: Single Turn [2/2]\", rt=1, rb=1, rw=80, rc='=', verbose=1):\n",
    "    output = ChatCompletion(prompt=prompt_chat, system_prompt=\"answer the last question in few words\")\n",
    "html(output)"
   ],
   "id": "8530c9c8b79f9eaf",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[05.01 14:17:51] ‚îá ================================================================================\n",
      "[05.01 14:17:51] ‚îá [INIT] 4.1 - Chat conversation: Single Turn [2/2]\n",
      "[05.01 14:17:51] ‚îá ================================================================================\n",
      "[05.01 14:17:52] ‚îá HTTP Request: POST https://api.replicate.com/v1/predictions \"HTTP/1.1 201 Created\"\n",
      "[05.01 14:17:52] ‚îá HTTP Request: GET https://api.replicate.com/v1/models/meta/llama-2-13b-chat/versions/f4e2de70d66816a838a89eeeb621910adffb0dd0baba3976c96980970978018d \"HTTP/1.1 200 OK\"\n",
      "[05.01 14:17:54] ‚îá HTTP Request: GET https://api.replicate.com/v1/predictions/svm0p6vjy9rgj0cf6ab8zmn864 \"HTTP/1.1 200 OK\"\n",
      "[05.01 14:17:54] ‚îá ================================================================================\n",
      "[05.01 14:17:54] ‚îá [EXIT] 4.1 - Chat conversation: Single Turn [2/2] ($=00:00:03.154)\n",
      "[05.01 14:17:54] ‚îá ================================================================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "<div style='font-family:Arial; font-size:12pt; padding:10px; margin-left:10px; margin-top:10px; background-color:LightSkyBlue; border:3px solid MidnightBlue;'> Sure! Here's the answer in few words:\n",
       "\n",
       "The animal family of the giant panda is the Ursidae family, which includes bears.</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Chat app requires us to send in previous context to LLM to get in valid responses. Below is an example of Multi-turn chat.",
   "id": "5ed6c2e295ea021b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-01T05:17:58.365076Z",
     "start_time": "2024-05-01T05:17:55.001697Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# example of multi-turn chat, with storing previous context\n",
    "prompt_chat = \"\"\"\n",
    "User: What is the average lifespan of a Llama?\n",
    "Assistant: Sure! The average lifespan of a llama is around 20-30 years.\n",
    "User: What animal family are they?\n",
    "\"\"\"\n",
    "with JobTimer(\"4.1 - Chat conversation: Multi Turn (Memory)\", rt=1, rb=1, rw=80, rc='=', verbose=1):\n",
    "    output = ChatCompletion(prompt=prompt_chat, system_prompt=\"answer the last question\")\n",
    "html(output)"
   ],
   "id": "8454cb422834ea24",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[05.01 14:17:55] ‚îá ================================================================================\n",
      "[05.01 14:17:55] ‚îá [INIT] 4.1 - Chat conversation: Multi Turn (Memory)\n",
      "[05.01 14:17:55] ‚îá ================================================================================\n",
      "[05.01 14:17:55] ‚îá HTTP Request: POST https://api.replicate.com/v1/predictions \"HTTP/1.1 201 Created\"\n",
      "[05.01 14:17:56] ‚îá HTTP Request: GET https://api.replicate.com/v1/models/meta/llama-2-13b-chat/versions/f4e2de70d66816a838a89eeeb621910adffb0dd0baba3976c96980970978018d \"HTTP/1.1 200 OK\"\n",
      "[05.01 14:17:57] ‚îá HTTP Request: GET https://api.replicate.com/v1/predictions/yr2rwpm2m5rgg0cf6ab9rqhny8 \"HTTP/1.1 200 OK\"\n",
      "[05.01 14:17:58] ‚îá ================================================================================\n",
      "[05.01 14:17:58] ‚îá [EXIT] 4.1 - Chat conversation: Multi Turn (Memory) ($=00:00:02.520)\n",
      "[05.01 14:17:58] ‚îá ================================================================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "<div style='font-family:Arial; font-size:12pt; padding:10px; margin-left:10px; margin-top:10px; background-color:LightSkyBlue; border:3px solid MidnightBlue;'> Sure! Llamas are members of the camelid family, which includes camels, alpacas, and vicu√±as.</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### **4.2 - Prompt Engineering**\n",
    "* Prompt engineering refers to the science of designing effective prompts to get desired responses\n",
    "* Helps reduce hallucination"
   ],
   "id": "5ae6ff4bba0e4d4"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### **4.2.1 - In-Context Learning (e.g. Zero-shot, Few-shot)**\n",
    "  * In-context learning - specific method of prompt engineering where demonstration of task are provided as part of prompt.\n",
    "  1. Zero-shot learning - model is performing tasks without any input examples.\n",
    "  2. Few or ‚ÄúN-Shot‚Äù Learning - model is performing and behaving based on input examples in user's prompt."
   ],
   "id": "195ccff05618fb37"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-01T05:18:02.054726Z",
     "start_time": "2024-05-01T05:17:58.368146Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Zero-shot example. To get positive/negative/neutral sentiment, we need to give examples in the prompt\n",
    "prompt = '''\n",
    "Classify: I saw a Gecko.\n",
    "Sentiment: ?\n",
    "'''\n",
    "with JobTimer(\"4.2.1 - In-Context Learning [1/4]\", rt=1, rb=1, rw=80, rc='=', verbose=1):\n",
    "    output = ChatCompletion(prompt, system_prompt=\"one word response\")\n",
    "html(output)"
   ],
   "id": "29b2d6ac41db8de",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[05.01 14:17:58] ‚îá ================================================================================\n",
      "[05.01 14:17:58] ‚îá [INIT] 4.2.1 - In-Context Learning [1/4]\n",
      "[05.01 14:17:58] ‚îá ================================================================================\n",
      "[05.01 14:17:59] ‚îá HTTP Request: POST https://api.replicate.com/v1/predictions \"HTTP/1.1 201 Created\"\n",
      "[05.01 14:18:00] ‚îá HTTP Request: GET https://api.replicate.com/v1/models/meta/llama-2-13b-chat/versions/f4e2de70d66816a838a89eeeb621910adffb0dd0baba3976c96980970978018d \"HTTP/1.1 200 OK\"\n",
      "[05.01 14:18:01] ‚îá HTTP Request: GET https://api.replicate.com/v1/predictions/98esvz4frdrgj0cf6ab8g1xwmw \"HTTP/1.1 200 OK\"\n",
      "[05.01 14:18:01] ‚îá ================================================================================\n",
      "[05.01 14:18:01] ‚îá [EXIT] 4.2.1 - In-Context Learning [1/4] ($=00:00:02.851)\n",
      "[05.01 14:18:01] ‚îá ================================================================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "<div style='font-family:Arial; font-size:12pt; padding:10px; margin-left:10px; margin-top:10px; background-color:LightSkyBlue; border:3px solid MidnightBlue;'> Cute</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-01T05:18:06.056793Z",
     "start_time": "2024-05-01T05:18:02.056809Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# By giving examples to Llama, it understands the expected output format.\n",
    "prompt = '''\n",
    "Classify: I love Llamas!\n",
    "Sentiment: Positive\n",
    "Classify: I dont like Snakes.\n",
    "Sentiment: Negative\n",
    "Classify: I saw a Gecko.\n",
    "Sentiment:'''\n",
    "with JobTimer(\"4.2.1 - In-Context Learning [2/4]\", rt=1, rb=1, rw=80, rc='=', verbose=1):\n",
    "    output = ChatCompletion(prompt, system_prompt=\"One word response\")\n",
    "html(output)"
   ],
   "id": "82e85255a9c01c7",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[05.01 14:18:02] ‚îá ================================================================================\n",
      "[05.01 14:18:02] ‚îá [INIT] 4.2.1 - In-Context Learning [2/4]\n",
      "[05.01 14:18:02] ‚îá ================================================================================\n",
      "[05.01 14:18:03] ‚îá HTTP Request: POST https://api.replicate.com/v1/predictions \"HTTP/1.1 201 Created\"\n",
      "[05.01 14:18:03] ‚îá HTTP Request: GET https://api.replicate.com/v1/models/meta/llama-2-13b-chat/versions/f4e2de70d66816a838a89eeeb621910adffb0dd0baba3976c96980970978018d \"HTTP/1.1 200 OK\"\n",
      "[05.01 14:18:05] ‚îá HTTP Request: GET https://api.replicate.com/v1/predictions/cwwrh5cy75rgj0cf6ab8768h1w \"HTTP/1.1 200 OK\"\n",
      "[05.01 14:18:05] ‚îá ================================================================================\n",
      "[05.01 14:18:05] ‚îá [EXIT] 4.2.1 - In-Context Learning [2/4] ($=00:00:03.152)\n",
      "[05.01 14:18:05] ‚îá ================================================================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "<div style='font-family:Arial; font-size:12pt; padding:10px; margin-left:10px; margin-top:10px; background-color:LightSkyBlue; border:3px solid MidnightBlue;'> Neutral</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-01T05:18:09.740326Z",
     "start_time": "2024-05-01T05:18:06.058493Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# another zero-shot learning\n",
    "prompt = '''\n",
    "QUESTION: Vicuna?\n",
    "ANSWER:'''\n",
    "with JobTimer(\"4.2.1 - In-Context Learning [3/4]\", rt=1, rb=1, rw=80, rc='=', verbose=1):\n",
    "    output = ChatCompletion(prompt, system_prompt=\"one word response\")\n",
    "html(output)"
   ],
   "id": "101bb2a57686a5f9",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[05.01 14:18:06] ‚îá ================================================================================\n",
      "[05.01 14:18:06] ‚îá [INIT] 4.2.1 - In-Context Learning [3/4]\n",
      "[05.01 14:18:06] ‚îá ================================================================================\n",
      "[05.01 14:18:07] ‚îá HTTP Request: POST https://api.replicate.com/v1/predictions \"HTTP/1.1 201 Created\"\n",
      "[05.01 14:18:08] ‚îá HTTP Request: GET https://api.replicate.com/v1/models/meta/llama-2-13b-chat/versions/f4e2de70d66816a838a89eeeb621910adffb0dd0baba3976c96980970978018d \"HTTP/1.1 200 OK\"\n",
      "[05.01 14:18:09] ‚îá HTTP Request: GET https://api.replicate.com/v1/predictions/sxymw6xdrxrgm0cf6ab98583mc \"HTTP/1.1 200 OK\"\n",
      "[05.01 14:18:09] ‚îá ================================================================================\n",
      "[05.01 14:18:09] ‚îá [EXIT] 4.2.1 - In-Context Learning [3/4] ($=00:00:02.833)\n",
      "[05.01 14:18:09] ‚îá ================================================================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "<div style='font-family:Arial; font-size:12pt; padding:10px; margin-left:10px; margin-top:10px; background-color:LightSkyBlue; border:3px solid MidnightBlue;'> Luxurious.</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-01T05:18:12.807221Z",
     "start_time": "2024-05-01T05:18:09.741869Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Another few-shot learning example with formatted prompt.\n",
    "prompt = '''\n",
    "QUESTION: Llama?\n",
    "ANSWER: Yes\n",
    "QUESTION: Alpaca?\n",
    "ANSWER: Yes\n",
    "QUESTION: Rabbit?\n",
    "ANSWER: No\n",
    "QUESTION: Vicuna?\n",
    "ANSWER:\n",
    "'''\n",
    "with JobTimer(\"4.2.1 - In-Context Learning [4/4]\", rt=1, rb=1, rw=80, rc='=', verbose=1):\n",
    "    output = ChatCompletion(prompt, system_prompt=\"one word response\")\n",
    "html(output)"
   ],
   "id": "e202cd8ba912d8cb",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[05.01 14:18:09] ‚îá ================================================================================\n",
      "[05.01 14:18:09] ‚îá [INIT] 4.2.1 - In-Context Learning [4/4]\n",
      "[05.01 14:18:09] ‚îá ================================================================================\n",
      "[05.01 14:18:10] ‚îá HTTP Request: POST https://api.replicate.com/v1/predictions \"HTTP/1.1 201 Created\"\n",
      "[05.01 14:18:11] ‚îá HTTP Request: GET https://api.replicate.com/v1/models/meta/llama-2-13b-chat/versions/f4e2de70d66816a838a89eeeb621910adffb0dd0baba3976c96980970978018d \"HTTP/1.1 200 OK\"\n",
      "[05.01 14:18:12] ‚îá HTTP Request: GET https://api.replicate.com/v1/predictions/jpahej5xtxrgg0cf6abbmc8nmc \"HTTP/1.1 200 OK\"\n",
      "[05.01 14:18:12] ‚îá ================================================================================\n",
      "[05.01 14:18:12] ‚îá [EXIT] 4.2.1 - In-Context Learning [4/4] ($=00:00:02.221)\n",
      "[05.01 14:18:12] ‚îá ================================================================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "<div style='font-family:Arial; font-size:12pt; padding:10px; margin-left:10px; margin-top:10px; background-color:LightSkyBlue; border:3px solid MidnightBlue;'> Sure! Here's one word response for each question:\n",
       "\n",
       "QUESTION: Llama?\n",
       "ANSWER: Yes\n",
       "QUESTION: Alpaca?\n",
       "ANSWER: Yes\n",
       "QUESTION: Rabbit?\n",
       "ANSWER: No\n",
       "QUESTION: Vicuna?\n",
       "ANSWER: Yes</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### **4.2.2 - Chain of Thought**\n",
    "\"Chain of thought\" enables complex reasoning through logical step by step thinking and generates meaningful and contextually relevant responses."
   ],
   "id": "8c0eb01f03c32d83"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-01T05:18:15.145230Z",
     "start_time": "2024-05-01T05:18:12.808052Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Standard prompting\n",
    "prompt = '''\n",
    "Llama started with 5 tennis balls. It buys 2 more cans of tennis balls. Each can has 3 tennis balls. How many tennis balls does Llama have now?\n",
    "'''\n",
    "with JobTimer(\"4.2.2 - Chain of Thought [1/2]\", rt=1, rb=1, rw=80, rc='=', verbose=1):\n",
    "    output = ChatCompletion(prompt, system_prompt=\"provide short answer\")\n",
    "html(output)"
   ],
   "id": "76484a2702ce9cf5",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[05.01 14:18:13] ‚îá ================================================================================\n",
      "[05.01 14:18:13] ‚îá [INIT] 4.2.2 - Chain of Thought [1/2]\n",
      "[05.01 14:18:13] ‚îá ================================================================================\n",
      "[05.01 14:18:13] ‚îá HTTP Request: POST https://api.replicate.com/v1/predictions \"HTTP/1.1 201 Created\"\n",
      "[05.01 14:18:13] ‚îá HTTP Request: GET https://api.replicate.com/v1/models/meta/llama-2-13b-chat/versions/f4e2de70d66816a838a89eeeb621910adffb0dd0baba3976c96980970978018d \"HTTP/1.1 200 OK\"\n",
      "[05.01 14:18:14] ‚îá HTTP Request: GET https://api.replicate.com/v1/predictions/zdem64y85drgg0cf6ab9jyxgmw \"HTTP/1.1 200 OK\"\n",
      "[05.01 14:18:14] ‚îá ================================================================================\n",
      "[05.01 14:18:14] ‚îá [EXIT] 4.2.2 - Chain of Thought [1/2] ($=00:00:01.500)\n",
      "[05.01 14:18:14] ‚îá ================================================================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "<div style='font-family:Arial; font-size:12pt; padding:10px; margin-left:10px; margin-top:10px; background-color:LightSkyBlue; border:3px solid MidnightBlue;'> Sure! Here's the answer to your question:\n",
       "\n",
       "Llama has 5 + 2 x 3 = 5 + 6 = 11 tennis balls now.</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-01T05:18:18.241391Z",
     "start_time": "2024-05-01T05:18:15.146170Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Chain-Of-Thought prompting\n",
    "prompt = '''\n",
    "Llama started with 5 tennis balls. It buys 2 more cans of tennis balls. Each can has 3 tennis balls. How many tennis balls does Llama have now?\n",
    "Let's think step by step.\n",
    "'''\n",
    "with JobTimer(\"4.2.2 - Chain of Thought [2/2]\", rt=1, rb=1, rw=80, rc='=', verbose=1):\n",
    "    output = ChatCompletion(prompt, system_prompt=\"provide short answer\")\n",
    "html(output)"
   ],
   "id": "c1aaa17e88044f92",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[05.01 14:18:15] ‚îá ================================================================================\n",
      "[05.01 14:18:15] ‚îá [INIT] 4.2.2 - Chain of Thought [2/2]\n",
      "[05.01 14:18:15] ‚îá ================================================================================\n",
      "[05.01 14:18:16] ‚îá HTTP Request: POST https://api.replicate.com/v1/predictions \"HTTP/1.1 201 Created\"\n",
      "[05.01 14:18:16] ‚îá HTTP Request: GET https://api.replicate.com/v1/models/meta/llama-2-13b-chat/versions/f4e2de70d66816a838a89eeeb621910adffb0dd0baba3976c96980970978018d \"HTTP/1.1 200 OK\"\n",
      "[05.01 14:18:17] ‚îá HTTP Request: GET https://api.replicate.com/v1/predictions/t33j50pjxsrgm0cf6ab9rhpmbr \"HTTP/1.1 200 OK\"\n",
      "[05.01 14:18:18] ‚îá ================================================================================\n",
      "[05.01 14:18:18] ‚îá [EXIT] 4.2.2 - Chain of Thought [2/2] ($=00:00:02.247)\n",
      "[05.01 14:18:18] ‚îá ================================================================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "<div style='font-family:Arial; font-size:12pt; padding:10px; margin-left:10px; margin-top:10px; background-color:LightSkyBlue; border:3px solid MidnightBlue;'> Sure! Here's the solution:\n",
       "\n",
       "Llama started with 5 tennis balls.\n",
       "It bought 2 more cans of tennis balls, and each can has 3 tennis balls.\n",
       "So, Llama now has 5 + 2 x 3 = 5 + 6 = 11 tennis balls. üê´üê´üéæ</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### **4.3 - Retrieval Augmented Generation (RAG)**\n",
    "* Prompt Eng Limitations - Knowledge cutoff & lack of specialized data\n",
    "* Retrieval Augmented Generation(RAG) allows us to retrieve snippets of information from external data sources and augment it to the user's prompt to get tailored responses from Llama 2.\n",
    "\n",
    "For our demo, we are going to download an external PDF file from a URL and query against the content in the pdf file to get contextually relevant information back with the help of Llama!"
   ],
   "id": "2c87637f7e017552"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-01T05:18:18.247553Z",
     "start_time": "2024-05-01T05:18:18.243136Z"
    }
   },
   "cell_type": "code",
   "source": "rag_arch()",
   "id": "17bd3c2b24aa01cc",
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://mermaid.ink/img/CiAgICBmbG93Y2hhcnQgVEQKICAgICAgICBBW1VzZXIgUHJvbXB0c10gLS0+IEIoRnJhbWV3b3JrcyBlLmcuIExhbmdDaGFpbikKICAgICAgICBCIDwtLT4gfERhdGFiYXNlLCBEb2NzLCBYTFN8Q1tmYTpmYS1kYXRhYmFzZSBFeHRlcm5hbCBEYXRhXQogICAgICAgIEIgLS0+fEFQSXxEW0xsYW1hIDJdCiAgICAgICAgY2xhc3NEZWYgZGVmYXVsdCBmaWxsOiNDQ0U2RkYsc3Ryb2tlOiM4NEJDRjUsdGV4dENvbG9yOiMxQzJCMzMsZm9udEZhbWlseTp0cmVidWNoZXQgbXM7CiAgICA=\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### **4.3.1 - LangChain**\n",
    "LangChain is a framework that helps make it easier to implement RAG."
   ],
   "id": "e41075a070214735"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-01T05:18:18.260352Z",
     "start_time": "2024-05-01T05:18:18.249380Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# langchain setup\n",
    "from langchain.llms import Replicate\n",
    "\n",
    "# Use the Llama 2 model hosted on Replicate\n",
    "# Temperature: Adjusts randomness of outputs, greater than 1 is random and 0 is deterministic, 0.75 is a good starting value\n",
    "# top_p: When decoding text, samples from the top p percentage of most likely tokens; lower to ignore less likely tokens\n",
    "# max_new_tokens: Maximum number of tokens to generate. A word is generally 2-3 tokens\n",
    "llama_model = Replicate(\n",
    "    model=model_ref,\n",
    "    model_kwargs={\"temperature\": 0.75, \"top_p\": 1, \"max_new_tokens\": 1000}\n",
    ")"
   ],
   "id": "9e2643146f26115b",
   "outputs": [],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-01T05:18:31.061547Z",
     "start_time": "2024-05-01T05:18:18.261317Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Step 1: load the external data source. In our case, we will load Meta‚Äôs ‚ÄúResponsible Use Guide‚Äù pdf document.\n",
    "loader = OnlinePDFLoader(\"https://ai.meta.com/static-resource/responsible-use-guide/\")\n",
    "document = loader.load()\n",
    "\n",
    "# Step 2: Get text splits from document\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=20)\n",
    "all_splits = text_splitter.split_documents(document)\n",
    "\n",
    "# Step 3: Use the embedding model\n",
    "model_name = \"sentence-transformers/all-mpnet-base-v2\"  # embedding model\n",
    "model_kwargs = {\"device\": \"cpu\"}\n",
    "model_embeddings = HuggingFaceEmbeddings(model_name=model_name, model_kwargs=model_kwargs)\n",
    "\n",
    "# Step 4: Use vector store to store embeddings\n",
    "document_vector = FAISS.from_documents(all_splits, model_embeddings)"
   ],
   "id": "d638881dea441cc8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[05.01 14:18:22] ‚îá pikepdf C++ to Python logger bridge initialized\n",
      "[05.01 14:18:24] ‚îá Load pretrained SentenceTransformer: sentence-transformers/all-mpnet-base-v2\n",
      "[05.01 14:18:31] ‚îá Loading faiss.\n",
      "[05.01 14:18:31] ‚îá Successfully loaded faiss.\n"
     ]
    }
   ],
   "execution_count": 22
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### **4.3.2 - LangChain Q&A Retriever**\n",
    "* ConversationalRetrievalChain\n",
    "* Query the Source documents"
   ],
   "id": "a3d7e1cf197ad834"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-01T05:18:37.088309Z",
     "start_time": "2024-05-01T05:18:31.062382Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Query against your own data\n",
    "chain = ConversationalRetrievalChain.from_llm(llama_model, document_vector.as_retriever(), return_source_documents=True)\n",
    "chat_history = []\n",
    "query = \"How is Meta approaching open science in two short sentences?\"\n",
    "with JobTimer(\"4.3.2 - LangChain Q&A Retriever [1/2]\", rt=1, rb=1, rw=80, rc='=', verbose=1):\n",
    "    result = chain.invoke({\"question\": query, \"chat_history\": chat_history})\n",
    "html(result['answer'])"
   ],
   "id": "8b8a69f63c4b1b08",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[05.01 14:18:31] ‚îá ================================================================================\n",
      "[05.01 14:18:31] ‚îá [INIT] 4.3.2 - LangChain Q&A Retriever [1/2]\n",
      "[05.01 14:18:31] ‚îá ================================================================================\n",
      "[05.01 14:18:33] ‚îá HTTP Request: GET https://api.replicate.com/v1/models/meta/llama-2-13b-chat \"HTTP/1.1 200 OK\"\n",
      "[05.01 14:18:33] ‚îá HTTP Request: GET https://api.replicate.com/v1/models/meta/llama-2-13b-chat/versions/f4e2de70d66816a838a89eeeb621910adffb0dd0baba3976c96980970978018d \"HTTP/1.1 200 OK\"\n",
      "[05.01 14:18:34] ‚îá HTTP Request: POST https://api.replicate.com/v1/predictions \"HTTP/1.1 201 Created\"\n",
      "[05.01 14:18:35] ‚îá HTTP Request: GET https://api.replicate.com/v1/predictions/xtwsx08rssrgj0cf6abtrdeycr \"HTTP/1.1 200 OK\"\n",
      "[05.01 14:18:36] ‚îá HTTP Request: GET https://api.replicate.com/v1/predictions/xtwsx08rssrgj0cf6abtrdeycr \"HTTP/1.1 200 OK\"\n",
      "[05.01 14:18:36] ‚îá ================================================================================\n",
      "[05.01 14:18:36] ‚îá [EXIT] 4.3.2 - LangChain Q&A Retriever [1/2] ($=00:00:05.180)\n",
      "[05.01 14:18:36] ‚îá ================================================================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "<div style='font-family:Arial; font-size:12pt; padding:10px; margin-left:10px; margin-top:10px; background-color:LightSkyBlue; border:3px solid MidnightBlue;'> Based on the provided context, here is a helpful answer to the question in two short sentences:\n",
       "\n",
       "Meta is committed to open science by democratizing access to AI technology and promoting collaboration on risk management. This includes open-sourcing code and datasets, supporting the developer community with tools like PyTorch and ONNX, and releasing models like Llama 3 for responsible use.</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-01T05:18:47.207013Z",
     "start_time": "2024-05-01T05:18:37.089986Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# This time your previous question and answer will be included as a chat history which will enable the ability\n",
    "# to ask follow up questions.\n",
    "chat_history = [(query, result[\"answer\"])]\n",
    "query = \"How is it benefiting the world?\"\n",
    "with JobTimer(\"4.3.2 - LangChain Q&A Retriever [2/2]\", rt=1, rb=1, rw=80, rc='=', verbose=1):\n",
    "    result = chain.invoke({\"question\": query, \"chat_history\": chat_history})\n",
    "html(result['answer'])"
   ],
   "id": "d7741a0aa701af69",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[05.01 14:18:37] ‚îá ================================================================================\n",
      "[05.01 14:18:37] ‚îá [INIT] 4.3.2 - LangChain Q&A Retriever [2/2]\n",
      "[05.01 14:18:37] ‚îá ================================================================================\n",
      "[05.01 14:18:39] ‚îá HTTP Request: POST https://api.replicate.com/v1/predictions \"HTTP/1.1 201 Created\"\n",
      "[05.01 14:18:40] ‚îá HTTP Request: GET https://api.replicate.com/v1/predictions/wazzsr99ahrgp0cf6abrmp42ew \"HTTP/1.1 200 OK\"\n",
      "[05.01 14:18:41] ‚îá HTTP Request: POST https://api.replicate.com/v1/predictions \"HTTP/1.1 201 Created\"\n",
      "[05.01 14:18:42] ‚îá HTTP Request: GET https://api.replicate.com/v1/predictions/k7exemhj69rgj0cf6absge24qm \"HTTP/1.1 200 OK\"\n",
      "[05.01 14:18:44] ‚îá HTTP Request: GET https://api.replicate.com/v1/predictions/k7exemhj69rgj0cf6absge24qm \"HTTP/1.1 200 OK\"\n",
      "[05.01 14:18:45] ‚îá HTTP Request: GET https://api.replicate.com/v1/predictions/k7exemhj69rgj0cf6absge24qm \"HTTP/1.1 200 OK\"\n",
      "[05.01 14:18:46] ‚îá HTTP Request: GET https://api.replicate.com/v1/predictions/k7exemhj69rgj0cf6absge24qm \"HTTP/1.1 200 OK\"\n",
      "[05.01 14:18:46] ‚îá ================================================================================\n",
      "[05.01 14:18:46] ‚îá [EXIT] 4.3.2 - LangChain Q&A Retriever [2/2] ($=00:00:09.277)\n",
      "[05.01 14:18:46] ‚îá ================================================================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "<div style='font-family:Arial; font-size:12pt; padding:10px; margin-left:10px; margin-top:10px; background-color:LightSkyBlue; border:3px solid MidnightBlue;'> Sure, I'd be happy to help answer that question!\n",
       "\n",
       "Meta's approach to open science is bringing several specific benefits to the world, and is positively impacting society and the scientific community in several ways.\n",
       "\n",
       "Firstly, by democratizing access to AI technology and collaboration on risk management, Meta is empowering developers in every industry on a global scale to drive breakthroughs, create new products and solutions, and benefit from accelerations in technological advancement and economic growth. This is leading to a vibrant AI-innovation ecosystem that is pushing the frontiers of scientific discovery and potentially revolutionizing a wide array of sectors, from education to agriculture, and climate management to cybersecurity.\n",
       "\n",
       "Secondly, by open sourcing code and datasets for machine translation, computer vision, and fairness evaluation, Meta is contributing to the infrastructure of the AI-developer community with tools like PyTorch, ONNX, Glow, and Detectron. This is making it easier than ever for developers to build and release models responsibly, and is helping to create a more transparent and controlled environment for the deployment of AI technology.\n",
       "\n",
       "Finally, by releasing the next generation of Llama, Meta Llama 3, which is licensed for commercial use, Meta is putting this technology in the hands of more people globally, which we believe is the right path to ensure that this technology will benefit the world at large.\n",
       "\n",
       "Overall, Meta's approach to open science is helping to drive breakthroughs, create new products and solutions, and benefit from accelerations in technological advancement and economic growth, while also promoting transparency, control, and responsible deployment of AI technology. This is positively impacting society and the scientific community, and is helping to create a more vibrant AI-innovation ecosystem.</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 24
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## **5 - Fine-Tuning Models**\n",
    "* Limitatons of Prompt Eng and RAG\n",
    "* Fine-Tuning Arch\n",
    "* Types (PEFT, LoRA, QLoRA)\n",
    "* Using PyTorch for Pre-Training & Fine-Tuning\n",
    "* Evals + Quality"
   ],
   "id": "485ed0b25f0baab6"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-01T05:18:47.210368Z",
     "start_time": "2024-05-01T05:18:47.207987Z"
    }
   },
   "cell_type": "code",
   "source": "fine_tuned_arch()",
   "id": "44df90baa5f9d992",
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://mermaid.ink/img/CiAgICBncmFwaCBMUjsKICAgICAgICBDdXN0b21fRGF0YXNldCAtLT4gUHJlLXRyYWluZWRfTGxhbWEKICAgICAgICBQcmUtdHJhaW5lZF9MbGFtYSAtLT4gRmluZS10dW5lZF9MbGFtYQogICAgICAgIEZpbmUtdHVuZWRfTGxhbWEgLS0+IFJMSEYKICAgICAgICBSTEhGIC0tPiB8TG9zczpDcm9zcy1FbnRyb3B5fEZpbmUtdHVuZWRfTGxhbWEKICAgICAgICBjbGFzc0RlZiBkZWZhdWx0IGZpbGw6I0NDRTZGRixzdHJva2U6Izg0QkNGNSx0ZXh0Q29sb3I6IzFDMkIzMyxmb250RmFtaWx5OnRyZWJ1Y2hldCBtczsKICAgIA==\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 25
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## **6 - Responsible AI**\n",
    "* Power + Responsibility\n",
    "* Hallucinations\n",
    "* Input & Output Safety\n",
    "* Red-teaming (simulating real-world cyber attackers)\n",
    "* [Responsible Use Guide](https://ai.meta.com/llama/responsible-use-guide/)"
   ],
   "id": "20d9ee189c8ef88d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## **7 - Conclusion**\n",
    "* Active research on LLMs and Llama\n",
    "* Leverage the power of Llama and its open community\n",
    "* Safety and responsible use is paramount!\n",
    "* Call-To-Action\n",
    "  * [Replicate Free Credits](https://replicate.fyi/connect2023) for Connect attendees!\n",
    "  * This notebook is available through Llama Github recipes\n",
    "  * Use Llama in your projects and give us feedback"
   ],
   "id": "8e65302a917a841b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### **Resources**\n",
    "- [GitHub - Llama 2](https://github.com/facebookresearch/llama)\n",
    "- [Github - LLama 2 Recipes](https://github.com/facebookresearch/llama-recipes)\n",
    "- [Llama 2](https://ai.meta.com/llama/)\n",
    "- [Research Paper](https://ai.meta.com/research/publications/llama-2-open-foundation-and-fine-tuned-chat-models/)\n",
    "- [Model Card](https://github.com/facebookresearch/llama/blob/main/MODEL_CARD.md)\n",
    "- [Responsible Use Guide](https://ai.meta.com/llama/responsible-use-guide/)\n",
    "- [Acceptable Use Policy](https://ai.meta.com/llama/use-policy/)\n",
    "- [Replicate](https://replicate.com/meta/)\n",
    "- [LangChain](https://www.langchain.com/)"
   ],
   "id": "d201a9aa70cfcad3"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### **Authors & Contact**\n",
    "  * asangani@meta.com, [Amit Sangani | LinkedIn](https://www.linkedin.com/in/amitsangani/)\n",
    "  * mohsena@meta.com, [Mohsen Agsen | LinkedIn](https://www.linkedin.com/in/mohsen-agsen-62a9791/)\n"
   ],
   "id": "bf0012709004082a"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
